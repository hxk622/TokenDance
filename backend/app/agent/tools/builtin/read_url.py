"""
Read URL å·¥å…·

æŠ“å–ç½‘é¡µå†…å®¹å¹¶è½¬æ¢ä¸º Markdown æ ¼å¼
æ”¯æŒ Jina Reader API å®ç° Token ä¼˜åŒ–ï¼ˆæ¨èï¼‰

Token ä¼˜åŒ–æ•ˆæœ:
- åŸå§‹ HTML: ~223K tokens
- å»é™¤ JS/CSS å: ~9K tokens
- Jina Reader Markdown: ~2-3K tokens (èŠ‚çœ 70-90%)
"""
import logging
import os
import re
from typing import Any

try:
    import httpx
    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False

try:
    from bs4 import BeautifulSoup
    BS4_AVAILABLE = True
except ImportError:
    BS4_AVAILABLE = False

try:
    import html2text
    HTML2TEXT_AVAILABLE = True
except ImportError:
    HTML2TEXT_AVAILABLE = False

from ..base import BaseTool
from ..risk import OperationCategory, RiskLevel

logger = logging.getLogger(__name__)

# Jina Reader API é…ç½®
JINA_READER_BASE_URL = "https://r.jina.ai/"
JINA_API_KEY = os.getenv("JINA_API_KEY")  # å¯é€‰ï¼Œå…è´¹ç”¨æ³•æ— éœ€ API Key


class ReadUrlTool(BaseTool):
    """ç½‘é¡µæŠ“å–å·¥å…·

    åŠŸèƒ½ï¼š
    - æŠ“å–ç½‘é¡µ HTML
    - æ¸…ç†æ— ç”¨å†…å®¹ï¼ˆè„šæœ¬ã€æ ·å¼ç­‰ï¼‰
    - è½¬æ¢ä¸º Markdown æ ¼å¼
    - æå–ä¸»è¦æ–‡æœ¬å†…å®¹
    - ã€æ¨èã€‘ä½¿ç”¨ Jina Reader API å®ç°é«˜æ•ˆ Token ä¼˜åŒ–

    Token ä¼˜åŒ–:
    - use_jina=True (é»˜è®¤): ä½¿ç”¨ Jina Reader APIï¼ŒèŠ‚çœ 70-90% tokens
    - use_jina=False: ä½¿ç”¨æœ¬åœ° html2text è½¬æ¢

    é£é™©ç­‰çº§ï¼šNONEï¼ˆçº¯è¯»å–æ“ä½œï¼Œæ— å‰¯ä½œç”¨ï¼‰
    """

    # é£é™©é…ç½®
    risk_level = RiskLevel.NONE
    operation_categories = [OperationCategory.WEB_READ]
    requires_confirmation = False

    def __init__(self):
        super().__init__(
            name="read_url",
            description=(
                "Fetch and read content from a web page URL. "
                "Converts HTML to clean, LLM-optimized Markdown text. "
                "Use this tool when you need to read detailed information from a specific web page. "
                "Automatically removes ads, navigation, and clutter for token efficiency."
            ),
            parameters_schema={
                "type": "object",
                "properties": {
                    "url": {
                        "type": "string",
                        "description": "The URL of the web page to fetch and read"
                    },
                    "max_length": {
                        "type": "integer",
                        "description": "Maximum content length in characters (default: 10000)",
                        "default": 10000,
                        "minimum": 1000,
                        "maximum": 50000
                    },
                    "use_jina": {
                        "type": "boolean",
                        "description": "Use Jina Reader API for better token optimization (default: true)",
                        "default": True
                    },
                    "extract_relevant": {
                        "type": "boolean",
                        "description": "Extract only query-relevant content using LLM (saves 60-80% more tokens)",
                        "default": False
                    },
                    "query": {
                        "type": "string",
                        "description": "The research query - used when extract_relevant=true to filter content"
                    }
                },
                "required": ["url"]
            },
            requires_confirmation=False
        )

        if not HTTPX_AVAILABLE:
            logger.warning("httpx not installed. URL reading will not work.")
        if not BS4_AVAILABLE:
            logger.warning("beautifulsoup4 not installed. HTML parsing will be limited.")
        if not HTML2TEXT_AVAILABLE:
            logger.warning("html2text not installed. Markdown conversion will be limited.")

    async def execute(self, **kwargs: Any) -> dict[str, Any]:
        """æ‰§è¡Œç½‘é¡µæŠ“å–

        Args:
            url: ç½‘é¡µ URL
            max_length: æœ€å¤§å†…å®¹é•¿åº¦ï¼ˆé»˜è®¤ 10000 å­—ç¬¦ï¼‰
            use_jina: æ˜¯å¦ä½¿ç”¨ Jina Reader APIï¼ˆé»˜è®¤ Trueï¼ŒèŠ‚çœ 70-90% tokensï¼‰
            extract_relevant: æ˜¯å¦åªæå–ä¸æŸ¥è¯¢ç›¸å…³çš„å†…å®¹ï¼ˆå†èŠ‚çœ 60-80% tokensï¼‰
            query: ç ”ç©¶æŸ¥è¯¢ï¼ˆextract_relevant=True æ—¶å¿…é¡»æä¾›ï¼‰

        Returns:
            Dict: æŠ“å–ç»“æœ
                - success: bool
                - url: str
                - title: str
                - content: str (Markdown æ ¼å¼)
                - length: int
                - method: str ("jina" or "local")
                - token_savings: str (ä¼°ç®—çš„ token èŠ‚çœ)
        """
        if not HTTPX_AVAILABLE:
            return {
                "success": False,
                "error": "httpx not installed. Install with: pip install httpx"
            }

        url = kwargs.get("url", "")
        max_length = kwargs.get("max_length", 10000)
        use_jina = kwargs.get("use_jina", True)  # é»˜è®¤ä½¿ç”¨ Jina
        extract_relevant = kwargs.get("extract_relevant", False)
        query = kwargs.get("query", "")

        if not url:
            return {
                "success": False,
                "error": "URL parameter is required"
            }

        # éªŒè¯ URL æ ¼å¼
        if not url.startswith(("http://", "https://")):
            url = "https://" + url

        logger.info(f"Reading URL: {url} (max_length={max_length}, use_jina={use_jina}, extract_relevant={extract_relevant})")

        try:
            if use_jina:
                # ä½¿ç”¨ Jina Reader API (æ¨èï¼ŒèŠ‚çœ token)
                result = await self._fetch_with_jina(url, max_length)
                if result.get("success"):
                    # å¦‚æœéœ€è¦æå–ç›¸å…³å†…å®¹
                    if extract_relevant and query:
                        result = await self._extract_relevant_content(result, query)
                    return result
                # Jina å¤±è´¥æ—¶å›é€€åˆ°æœ¬åœ°æ–¹æ³•
                logger.warning(f"Jina Reader failed, falling back to local: {result.get('error')}")

            # æœ¬åœ°æ–¹æ³•: å¼‚æ­¥æŠ“å–ç½‘é¡µ
            html_content = await self._fetch_html(url)

            # è§£æå’Œæ¸…ç† HTML
            title, clean_text = self._parse_html(html_content)

            # è½¬æ¢ä¸º Markdown
            markdown_content = self._html_to_markdown(clean_text)

            # æˆªæ–­å†…å®¹
            if len(markdown_content) > max_length:
                markdown_content = markdown_content[:max_length] + "\n\n... (content truncated)"

            logger.info(f"Successfully read URL (local): {url} ({len(markdown_content)} chars)")

            return {
                "success": True,
                "url": url,
                "title": title,
                "content": markdown_content,
                "length": len(markdown_content),
                "method": "local",
                "token_savings": "~50% (local html2text)"
            }

        except Exception as e:
            logger.error(f"Failed to read URL {url}: {e}", exc_info=True)
            return {
                "success": False,
                "url": url,
                "error": str(e)
            }

    async def _fetch_with_jina(self, url: str, max_length: int) -> dict[str, Any]:
        """ä½¿ç”¨ Jina Reader API è·å–å¹²å‡€çš„ Markdown

        Jina Reader API:
        - å…è´¹åŸºç¡€ç”¨æ³•: åœ¨ URL å‰åŠ  https://r.jina.ai/
        - è‡ªåŠ¨ç§»é™¤å¹¿å‘Šã€å¯¼èˆªã€è„šæœ¬
        - è¾“å‡ºå¹²å‡€çš„ LLM-friendly Markdown
        - èŠ‚çœ 70-90% tokens

        Args:
            url: åŸå§‹ URL
            max_length: æœ€å¤§å†…å®¹é•¿åº¦

        Returns:
            Dict: ç»“æœ
        """
        jina_url = f"{JINA_READER_BASE_URL}{url}"

        headers = {
            "Accept": "text/markdown",
            "User-Agent": "TokenDance/1.0 (Deep Research Agent)"
        }

        # å¦‚æœæœ‰ API Keyï¼Œæ·»åŠ åˆ°è¯·æ±‚å¤´
        if JINA_API_KEY:
            headers["Authorization"] = f"Bearer {JINA_API_KEY}"

        try:
            async with httpx.AsyncClient(timeout=60.0, follow_redirects=True) as client:
                response = await client.get(jina_url, headers=headers)
                response.raise_for_status()

                content = response.text

                # æå–æ ‡é¢˜ (é€šå¸¸åœ¨ç¬¬ä¸€è¡Œ)
                lines = content.strip().split('\n')
                title = "No title"
                if lines and lines[0].startswith('# '):
                    title = lines[0][2:].strip()
                elif lines and lines[0].startswith('Title: '):
                    title = lines[0][7:].strip()

                # æˆªæ–­å†…å®¹
                if len(content) > max_length:
                    content = content[:max_length] + "\n\n... (content truncated)"

                logger.info(f"Successfully read URL (Jina): {url} ({len(content)} chars)")

                return {
                    "success": True,
                    "url": url,
                    "title": title,
                    "content": content,
                    "length": len(content),
                    "method": "jina",
                    "token_savings": "~70-90% (Jina Reader)"
                }

        except httpx.HTTPStatusError as e:
            logger.warning(f"Jina Reader HTTP error for {url}: {e.response.status_code}")
            return {
                "success": False,
                "error": f"Jina Reader HTTP {e.response.status_code}: {str(e)}"
            }
        except Exception as e:
            logger.warning(f"Jina Reader error for {url}: {e}")
            return {
                "success": False,
                "error": f"Jina Reader error: {str(e)}"
            }

    async def _extract_relevant_content(
        self,
        result: dict[str, Any],
        query: str
    ) -> dict[str, Any]:
        """ä½¿ç”¨ LLM æå–ä¸æŸ¥è¯¢ç›¸å…³çš„å†…å®¹

        Index-based Extraction æ€æƒ³:
        - å°†å†…å®¹åˆ†å—
        - ç”¨ LLM è¯†åˆ«ç›¸å…³å—çš„ç´¢å¼•
        - åªè¿”å›ç›¸å…³å—
        - èŠ‚çœ 60-80% tokens

        Args:
            result: åŸå§‹æ‹“å–ç»“æœ
            query: ç ”ç©¶æŸ¥è¯¢

        Returns:
            Dict: è¿‡æ»¤åçš„ç»“æœ
        """
        content = result.get("content", "")
        if not content or len(content) < 500:
            # å†…å®¹å¤ªçŸ­ï¼Œæ— éœ€è¿‡æ»¤
            return result

        # å°†å†…å®¹åˆ†å— (æŒ‰æ®µè½/æ ‡é¢˜)
        chunks = self._split_into_chunks(content)
        if len(chunks) <= 3:
            # å—æ•°å¤ªå°‘ï¼Œæ— éœ€è¿‡æ»¤
            return result

        # æ„å»ºç´¢å¼•æç¤º
        "\n".join([
            f"[{i}] {chunk[:200]}..." if len(chunk) > 200 else f"[{i}] {chunk}"
            for i, chunk in enumerate(chunks)
        ])

        # ä½¿ç”¨ç®€å•çš„å…³é”®è¯åŒ¹é… (èŠ‚çœ LLM è°ƒç”¨)
        # å¦‚æœéœ€è¦æ›´ç²¾ç¡®çš„æå–ï¼Œå¯ä»¥è°ƒç”¨ LLM
        relevant_indices = self._find_relevant_indices_fast(chunks, query)

        if not relevant_indices:
            # æ‰¾ä¸åˆ°ç›¸å…³å†…å®¹ï¼Œè¿”å›åŸå§‹ç»“æœçš„å‰éƒ¨åˆ†
            logger.warning(f"No relevant content found for query: {query[:50]}")
            return result

        # é‡ç»„ç›¸å…³å†…å®¹
        relevant_content = "\n\n".join([chunks[i] for i in relevant_indices])

        original_len = len(content)
        filtered_len = len(relevant_content)
        savings = round((1 - filtered_len / original_len) * 100)

        logger.info(
            f"Query-Relevant extraction: {original_len} -> {filtered_len} chars "
            f"({savings}% reduction, {len(relevant_indices)}/{len(chunks)} chunks)"
        )

        return {
            **result,
            "content": relevant_content,
            "length": filtered_len,
            "extraction_method": "query_relevant",
            "token_savings": f"~{savings}% (query-relevant extraction)",
            "chunks_kept": len(relevant_indices),
            "chunks_total": len(chunks)
        }

    def _split_into_chunks(self, content: str) -> list[str]:
        """å°†å†…å®¹åˆ†å—

        æŒ‰ä¼˜å…ˆçº§åˆ†å‰²:
        1. Markdown æ ‡é¢˜ (##)
        2. ç©ºè¡Œ
        3. å›ºå®šé•¿åº¦
        """
        # å…ˆæŒ‰ Markdown æ ‡é¢˜åˆ†å‰²
        import re

        # åŒ¹é… Markdown æ ‡é¢˜
        header_pattern = r'\n(?=#{1,3}\s)'
        chunks = re.split(header_pattern, content)

        # å¦‚æœåˆ†å—å¤ªå°‘ï¼ŒæŒ‰åŒæ¢è¡Œåˆ†å‰²
        if len(chunks) < 5:
            chunks = content.split('\n\n')

        # è¿‡æ»¤ç©ºå—å’Œå¤ªçŸ­çš„å—
        chunks = [c.strip() for c in chunks if c.strip() and len(c.strip()) > 50]

        # å¦‚æœå—å¤ªå¤§ï¼Œè¿›ä¸€æ­¥åˆ†å‰²
        final_chunks = []
        for chunk in chunks:
            if len(chunk) > 1500:
                # æŒ‰å¥å­åˆ†å‰²
                sentences = re.split(r'(?<=[.!?ã€‚ï¼ï¼Ÿ])\s+', chunk)
                current = ""
                for sent in sentences:
                    if len(current) + len(sent) < 1200:
                        current += sent + " "
                    else:
                        if current:
                            final_chunks.append(current.strip())
                        current = sent + " "
                if current:
                    final_chunks.append(current.strip())
            else:
                final_chunks.append(chunk)

        return final_chunks if final_chunks else chunks

    def _find_relevant_indices_fast(self, chunks: list[str], query: str) -> list[int]:
        """å¿«é€ŸæŸ¥æ‰¾ç›¸å…³å—ç´¢å¼• (åŸºäºå…³é”®è¯åŒ¹é…)

        ä¸è°ƒç”¨ LLMï¼Œä½¿ç”¨ç®€å•çš„å…³é”®è¯åŒ¹é… + è¯„åˆ†
        """
        import re

        # æå–æŸ¥è¯¢å…³é”®è¯ (ç§»é™¤åœç”¨è¯)
        stop_words = {
            'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been',
            'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',
            'could', 'should', 'may', 'might', 'must', 'shall',
            'this', 'that', 'these', 'those', 'it', 'its',
            'and', 'or', 'but', 'if', 'then', 'else', 'when', 'where',
            'what', 'which', 'who', 'whom', 'how', 'why',
            'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from',
            'çš„', 'æ˜¯', 'åœ¨', 'äº†', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†',
            'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'å“ªä¸ª', 'è¿™ä¸ª', 'é‚£ä¸ª'
        }

        # æå–æŸ¥è¯¢å…³é”®è¯
        query_words = set()
        for word in re.findall(r'\w+', query.lower()):
            if word not in stop_words and len(word) > 2:
                query_words.add(word)

        # æ·»åŠ ä¸­æ–‡å…³é”®è¯ (æŒ‰å­—ç¬¦)
        for char in query:
            if '\u4e00' <= char <= '\u9fff':  # ä¸­æ–‡å­—ç¬¦
                query_words.add(char)

        if not query_words:
            # æ— å…³é”®è¯ï¼Œè¿”å›å‰å‡ å—
            return list(range(min(5, len(chunks))))

        # è®¡ç®—æ¯å—çš„ç›¸å…³æ€§å¾—åˆ†
        scores = []
        for i, chunk in enumerate(chunks):
            chunk_lower = chunk.lower()
            score = 0

            # å…³é”®è¯åŒ¹é…
            for word in query_words:
                count = chunk_lower.count(word)
                if count > 0:
                    score += count * (2 if len(word) > 4 else 1)

            # æ ‡é¢˜åŠ åˆ† (åŒ…å« # çš„å—)
            if chunk.strip().startswith('#'):
                score *= 1.5

            # åˆ—è¡¨/æ•°æ®åŠ åˆ†
            if re.search(r'\d+[%\.\$ä¸‡äº¿]|\|.*\||\*\s+', chunk):
                score *= 1.2

            scores.append((i, score))

        # æ’åºå¹¶ç­›é€‰
        scores.sort(key=lambda x: x[1], reverse=True)

        # å–å¾—åˆ† > 0 çš„å—ï¼Œæœ€å¤šå– 10 å—
        relevant = [i for i, score in scores if score > 0][:10]

        # æŒ‰åŸå§‹é¡ºåºæ’åˆ—
        relevant.sort()

        # å¦‚æœæ²¡æœ‰ç›¸å…³å—ï¼Œè¿”å›å‰ 5 å—
        if not relevant:
            return list(range(min(5, len(chunks))))

        return relevant

    async def _fetch_html(self, url: str) -> str:
        """å¼‚æ­¥æŠ“å–ç½‘é¡µ HTML

        Args:
            url: ç½‘é¡µ URL

        Returns:
            str: HTML å†…å®¹
        """
        async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
            response = await client.get(
                url,
                headers={
                    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
                }
            )
            response.raise_for_status()
            return response.text

    def _parse_html(self, html: str) -> tuple[str, str]:
        """è§£æå’Œæ¸…ç† HTML

        Args:
            html: åŸå§‹ HTML

        Returns:
            tuple: (title, clean_text)
        """
        if not BS4_AVAILABLE:
            # ç®€å•çš„æ–‡æœ¬æå–ï¼ˆæ—  BeautifulSoupï¼‰
            title = re.search(r'<title>(.*?)</title>', html, re.IGNORECASE)
            title = title.group(1) if title else "No title"

            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            text = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
            text = re.sub(r'<style[^>]*>.*?</style>', '', text, flags=re.DOTALL | re.IGNORECASE)
            text = re.sub(r'<[^>]+>', '', text)  # ç§»é™¤æ‰€æœ‰æ ‡ç­¾

            return title, text

        # ä½¿ç”¨ BeautifulSoup è§£æ
        soup = BeautifulSoup(html, 'html.parser')

        # æå–æ ‡é¢˜
        title = soup.title.string if soup.title else "No title"

        # ç§»é™¤æ— ç”¨å…ƒç´ 
        for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'iframe']):
            element.decompose()

        # æå–ä¸»è¦å†…å®¹åŒºåŸŸ
        main_content = (
            soup.find('article') or
            soup.find('main') or
            soup.find('div', class_=re.compile('content|main|article', re.I)) or
            soup.body or
            soup
        )

        # è·å–æ¸…ç†åçš„ HTML
        clean_html = str(main_content)

        return title.strip(), clean_html

    def _html_to_markdown(self, html: str) -> str:
        """è½¬æ¢ HTML ä¸º Markdown

        Args:
            html: HTML å†…å®¹

        Returns:
            str: Markdown æ–‡æœ¬
        """
        if not HTML2TEXT_AVAILABLE:
            # ç®€å•çš„çº¯æ–‡æœ¬æå–
            text = re.sub(r'<[^>]+>', '', html)
            text = re.sub(r'\s+', ' ', text)
            return text.strip()

        # ä½¿ç”¨ html2text è½¬æ¢
        h = html2text.HTML2Text()
        h.ignore_links = False
        h.ignore_images = True
        h.ignore_emphasis = False
        h.body_width = 0  # ä¸æ¢è¡Œ

        markdown = h.handle(html)

        # æ¸…ç†å¤šä½™ç©ºè¡Œ
        markdown = re.sub(r'\n{3,}', '\n\n', markdown)

        return markdown.strip()

    def format_result(self, result: dict[str, Any]) -> str:
        """æ ¼å¼åŒ–ç½‘é¡µå†…å®¹ä¸ºå¯è¯»æ–‡æœ¬

        Args:
            result: execute() è¿”å›çš„ç»“æœ

        Returns:
            str: æ ¼å¼åŒ–çš„æ–‡æœ¬
        """
        if not result.get("success"):
            error = result.get("error", "Unknown error")
            url = result.get("url", "")
            return f"âŒ Failed to read URL: {url}\nError: {error}"

        url = result.get("url", "")
        title = result.get("title", "No title")
        content = result.get("content", "")
        length = result.get("length", 0)

        formatted = f"ğŸ“„ **{title}**\n"
        formatted += f"ğŸ”— {url}\n"
        formatted += f"ğŸ“ {length} characters\n\n"
        formatted += "---\n\n"
        formatted += content

        return formatted


# ä¾¿æ·å‡½æ•°
def create_read_url_tool() -> ReadUrlTool:
    """åˆ›å»º read_url å·¥å…·å®ä¾‹

    Returns:
        ReadUrlTool: URL è¯»å–å·¥å…·å®ä¾‹
    """
    return ReadUrlTool()
