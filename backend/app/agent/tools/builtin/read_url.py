# -*- coding: utf-8 -*-
"""
Read URL å·¥å…·

æŠ“å–ç½‘é¡µå†…å®¹å¹¶è½¬æ¢ä¸º Markdown æ ¼å¼
"""
import logging
from typing import Any, Dict
import asyncio
import re

try:
    import httpx
    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False

try:
    from bs4 import BeautifulSoup
    BS4_AVAILABLE = True
except ImportError:
    BS4_AVAILABLE = False

try:
    import html2text
    HTML2TEXT_AVAILABLE = True
except ImportError:
    HTML2TEXT_AVAILABLE = False

from ..base import BaseTool

logger = logging.getLogger(__name__)


class ReadUrlTool(BaseTool):
    """ç½‘é¡µæŠ“å–å·¥å…·
    
    åŠŸèƒ½ï¼š
    - æŠ“å–ç½‘é¡µ HTML
    - æ¸…ç†æ— ç”¨å†…å®¹ï¼ˆè„šæœ¬ã€æ ·å¼ç­‰ï¼‰
    - è½¬æ¢ä¸º Markdown æ ¼å¼
    - æå–ä¸»è¦æ–‡æœ¬å†…å®¹
    """
    
    def __init__(self):
        super().__init__(
            name="read_url",
            description=(
                "Fetch and read content from a web page URL. "
                "Converts HTML to clean Markdown text. "
                "Use this tool when you need to read detailed information from a specific web page."
            ),
            parameters_schema={
                "type": "object",
                "properties": {
                    "url": {
                        "type": "string",
                        "description": "The URL of the web page to fetch and read"
                    },
                    "max_length": {
                        "type": "integer",
                        "description": "Maximum content length in characters (default: 10000)",
                        "default": 10000,
                        "minimum": 1000,
                        "maximum": 50000
                    }
                },
                "required": ["url"]
            },
            requires_confirmation=False
        )
        
        if not HTTPX_AVAILABLE:
            logger.warning("httpx not installed. URL reading will not work.")
        if not BS4_AVAILABLE:
            logger.warning("beautifulsoup4 not installed. HTML parsing will be limited.")
        if not HTML2TEXT_AVAILABLE:
            logger.warning("html2text not installed. Markdown conversion will be limited.")
    
    async def execute(self, **kwargs: Any) -> Dict[str, Any]:
        """æ‰§è¡Œç½‘é¡µæŠ“å–
        
        Args:
            url: ç½‘é¡µ URL
            max_length: æœ€å¤§å†…å®¹é•¿åº¦ï¼ˆé»˜è®¤ 10000 å­—ç¬¦ï¼‰
            
        Returns:
            Dict: æŠ“å–ç»“æœ
                - success: bool
                - url: str
                - title: str
                - content: str (Markdown æ ¼å¼)
                - length: int
        """
        if not HTTPX_AVAILABLE:
            return {
                "success": False,
                "error": "httpx not installed. Install with: pip install httpx"
            }
        
        url = kwargs.get("url", "")
        max_length = kwargs.get("max_length", 10000)
        
        if not url:
            return {
                "success": False,
                "error": "URL parameter is required"
            }
        
        # éªŒè¯ URL æ ¼å¼
        if not url.startswith(("http://", "https://")):
            url = "https://" + url
        
        logger.info(f"Reading URL: {url} (max_length={max_length})")
        
        try:
            # å¼‚æ­¥æŠ“å–ç½‘é¡µ
            html_content = await self._fetch_html(url)
            
            # è§£æå’Œæ¸…ç† HTML
            title, clean_text = self._parse_html(html_content)
            
            # è½¬æ¢ä¸º Markdown
            markdown_content = self._html_to_markdown(clean_text)
            
            # æˆªæ–­å†…å®¹
            if len(markdown_content) > max_length:
                markdown_content = markdown_content[:max_length] + "\n\n... (content truncated)"
            
            logger.info(f"Successfully read URL: {url} ({len(markdown_content)} chars)")
            
            return {
                "success": True,
                "url": url,
                "title": title,
                "content": markdown_content,
                "length": len(markdown_content)
            }
        
        except Exception as e:
            logger.error(f"Failed to read URL {url}: {e}", exc_info=True)
            return {
                "success": False,
                "url": url,
                "error": str(e)
            }
    
    async def _fetch_html(self, url: str) -> str:
        """å¼‚æ­¥æŠ“å–ç½‘é¡µ HTML
        
        Args:
            url: ç½‘é¡µ URL
            
        Returns:
            str: HTML å†…å®¹
        """
        async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
            response = await client.get(
                url,
                headers={
                    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
                }
            )
            response.raise_for_status()
            return response.text
    
    def _parse_html(self, html: str) -> tuple[str, str]:
        """è§£æå’Œæ¸…ç† HTML
        
        Args:
            html: åŸå§‹ HTML
            
        Returns:
            tuple: (title, clean_text)
        """
        if not BS4_AVAILABLE:
            # ç®€å•çš„æ–‡æœ¬æå–ï¼ˆæ—  BeautifulSoupï¼‰
            title = re.search(r'<title>(.*?)</title>', html, re.IGNORECASE)
            title = title.group(1) if title else "No title"
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            text = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
            text = re.sub(r'<style[^>]*>.*?</style>', '', text, flags=re.DOTALL | re.IGNORECASE)
            text = re.sub(r'<[^>]+>', '', text)  # ç§»é™¤æ‰€æœ‰æ ‡ç­¾
            
            return title, text
        
        # ä½¿ç”¨ BeautifulSoup è§£æ
        soup = BeautifulSoup(html, 'html.parser')
        
        # æå–æ ‡é¢˜
        title = soup.title.string if soup.title else "No title"
        
        # ç§»é™¤æ— ç”¨å…ƒç´ 
        for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'iframe']):
            element.decompose()
        
        # æå–ä¸»è¦å†…å®¹åŒºåŸŸ
        main_content = (
            soup.find('article') or
            soup.find('main') or
            soup.find('div', class_=re.compile('content|main|article', re.I)) or
            soup.body or
            soup
        )
        
        # è·å–æ¸…ç†åçš„ HTML
        clean_html = str(main_content)
        
        return title.strip(), clean_html
    
    def _html_to_markdown(self, html: str) -> str:
        """è½¬æ¢ HTML ä¸º Markdown
        
        Args:
            html: HTML å†…å®¹
            
        Returns:
            str: Markdown æ–‡æœ¬
        """
        if not HTML2TEXT_AVAILABLE:
            # ç®€å•çš„çº¯æ–‡æœ¬æå–
            text = re.sub(r'<[^>]+>', '', html)
            text = re.sub(r'\s+', ' ', text)
            return text.strip()
        
        # ä½¿ç”¨ html2text è½¬æ¢
        h = html2text.HTML2Text()
        h.ignore_links = False
        h.ignore_images = True
        h.ignore_emphasis = False
        h.body_width = 0  # ä¸æ¢è¡Œ
        
        markdown = h.handle(html)
        
        # æ¸…ç†å¤šä½™ç©ºè¡Œ
        markdown = re.sub(r'\n{3,}', '\n\n', markdown)
        
        return markdown.strip()
    
    def format_result(self, result: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–ç½‘é¡µå†…å®¹ä¸ºå¯è¯»æ–‡æœ¬
        
        Args:
            result: execute() è¿”å›çš„ç»“æœ
            
        Returns:
            str: æ ¼å¼åŒ–çš„æ–‡æœ¬
        """
        if not result.get("success"):
            error = result.get("error", "Unknown error")
            url = result.get("url", "")
            return f"âŒ Failed to read URL: {url}\nError: {error}"
        
        url = result.get("url", "")
        title = result.get("title", "No title")
        content = result.get("content", "")
        length = result.get("length", 0)
        
        formatted = f"ğŸ“„ **{title}**\n"
        formatted += f"ğŸ”— {url}\n"
        formatted += f"ğŸ“ {length} characters\n\n"
        formatted += "---\n\n"
        formatted += content
        
        return formatted


# ä¾¿æ·å‡½æ•°
def create_read_url_tool() -> ReadUrlTool:
    """åˆ›å»º read_url å·¥å…·å®ä¾‹
    
    Returns:
        ReadUrlTool: URL è¯»å–å·¥å…·å®ä¾‹
    """
    return ReadUrlTool()
