"""
Web Search å·¥å…·

æ”¯æŒå¤šæœç´¢æºè‡ªåŠ¨é™çº§:
1. Serper (Google ç»“æœ, éœ€ API Key, æœ€ç¨³å®š)
2. Brave Search (éœ€ API Key)
3. DuckDuckGo (å…è´¹, æ— éœ€ API Key)
4. httpx å¤‡é€‰ (è§£å†³ SSL é—®é¢˜)
"""
import asyncio
import logging
import re
from typing import Any
from urllib.parse import quote_plus

try:
    from duckduckgo_search import DDGS
    DDGS_AVAILABLE = True
except ImportError:
    DDGS_AVAILABLE = False

try:
    import httpx
    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False

# å¯¼å…¥å¤šæºæœç´¢å™¨
try:
    from .search_providers import get_multi_source_searcher
    MULTI_SOURCE_AVAILABLE = True
except ImportError:
    MULTI_SOURCE_AVAILABLE = False

from ..base import BaseTool
from ..risk import OperationCategory, RiskLevel

logger = logging.getLogger(__name__)


class WebSearchTool(BaseTool):
    """ç½‘é¡µæœç´¢å·¥å…·

    ä½¿ç”¨ DuckDuckGo æœç´¢å¼•æ“è¿›è¡Œç½‘é¡µæœç´¢ã€‚
    å…è´¹ã€æ— éœ€ API Keyã€æ”¯æŒä¸­è‹±æ–‡ã€‚

    åŠŸèƒ½ï¼š
    - å…³é”®è¯æœç´¢
    - è¿”å›æ ‡é¢˜ã€é“¾æ¥ã€æ‘˜è¦
    - å¯é…ç½®ç»“æœæ•°é‡

    é£é™©ç­‰çº§ï¼šNONEï¼ˆçº¯è¯»å–æ“ä½œï¼Œæ— å‰¯ä½œç”¨ï¼‰
    """

    # å·¥å…·å®šä¹‰ï¼ˆç±»å±æ€§ï¼‰
    name = "web_search"
    description = (
        "Search the web for information using DuckDuckGo. "
        "Returns a list of search results with titles, links, and snippets. "
        "Use this tool when you need to find current information, "
        "research a topic, or verify facts from the internet."
    )
    parameters = {
        "type": "object",
        "properties": {
            "query": {
                "type": "string",
                "description": "The search query string. Be specific and concise."
            },
            "max_results": {
                "type": "integer",
                "description": "Maximum number of results to return (default: 5)",
                "default": 5,
                "minimum": 1,
                "maximum": 10
            },
            "region": {
                "type": "string",
                "description": "Region code for search results (e.g., 'cn-zh' for China, 'us-en' for US)",
                "default": "wt-wt"
            }
        },
        "required": ["query"]
    }

    # é£é™©é…ç½®
    risk_level = RiskLevel.NONE
    operation_categories = [OperationCategory.WEB_SEARCH]
    requires_confirmation = False

    def __init__(self):
        super().__init__()
        if not DDGS_AVAILABLE:
            logger.warning("duckduckgo-search not installed. Web search will not work.")

    async def execute(self, **kwargs: Any) -> dict[str, Any]:
        """æ‰§è¡Œç½‘é¡µæœç´¢

        Args:
            query: æœç´¢å…³é”®è¯
            max_results: æœ€å¤§ç»“æœæ•°ï¼ˆé»˜è®¤ 5ï¼‰
            region: åœ°åŒºä»£ç ï¼ˆé»˜è®¤ 'wt-wt' å…¨çƒï¼‰

        Returns:
            Dict: æœç´¢ç»“æœ
                - success: bool
                - results: List[Dict]
                    - title: str
                    - link: str
                    - snippet: str
                - query: str
                - count: int
        """
        if not DDGS_AVAILABLE:
            return {
                "success": False,
                "error": "duckduckgo-search not installed. Install with: pip install duckduckgo-search",
                "results": []
            }

        query = kwargs.get("query", "")
        max_results = kwargs.get("max_results", 5)
        region = kwargs.get("region", "wt-wt")

        if not query:
            return {
                "success": False,
                "error": "Query parameter is required",
                "results": []
            }

        logger.info(f"Searching web: '{query}' (max_results={max_results}, region={region})")

        # ä¼˜å…ˆä½¿ç”¨å¤šæºæœç´¢å™¨ (Serper/Brave/DuckDuckGo è‡ªåŠ¨é™çº§)
        if MULTI_SOURCE_AVAILABLE:
            try:
                searcher = get_multi_source_searcher()
                result = await searcher.search(query, max_results)

                if result.get("success"):
                    logger.info(
                        f"Found {len(result.get('results', []))} results via {result.get('provider')} "
                        f"(fallback: {result.get('fallback_used')})"
                    )
                    return {
                        "success": True,
                        "query": query,
                        "count": len(result.get("results", [])),
                        "results": result.get("results", []),
                        "provider": result.get("provider")
                    }
                else:
                    logger.warning(f"MultiSourceSearcher failed: {result.get('errors')}")
            except Exception as e:
                logger.warning(f"MultiSourceSearcher error: {e}")

        # å¤‡é€‰: ç›´æ¥ä½¿ç”¨ DuckDuckGo æˆ– httpx
        try:
            loop = asyncio.get_event_loop()
            results = await loop.run_in_executor(
                None,
                self._search_sync,
                query,
                max_results,
                region
            )

            logger.info(f"Found {len(results)} results for query: '{query}'")

            return {
                "success": True,
                "query": query,
                "count": len(results),
                "results": results
            }

        except Exception as e:
            logger.error(f"Web search failed: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "query": query,
                "results": []
            }

    def _search_sync(self, query: str, max_results: int, region: str) -> list:
        """åŒæ­¥æœç´¢æ–¹æ³•ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­è°ƒç”¨ï¼‰

        Args:
            query: æœç´¢å…³é”®è¯
            max_results: æœ€å¤§ç»“æœæ•°
            region: åœ°åŒºä»£ç 

        Returns:
            List[Dict]: æœç´¢ç»“æœåˆ—è¡¨
        """
        # å°è¯•ä½¿ç”¨ DDGS
        if DDGS_AVAILABLE:
            try:
                with DDGS() as ddgs:
                    raw_results = ddgs.text(
                        keywords=query,
                        region=region,
                        safesearch='moderate',
                        max_results=max_results
                    )

                    formatted_results = []
                    for result in raw_results:
                        formatted_results.append({
                            "title": result.get("title", ""),
                            "link": result.get("href", ""),
                            "snippet": result.get("body", "")
                        })

                    return formatted_results

            except Exception as e:
                logger.warning(f"DDGS search failed, trying httpx fallback: {e}")

        # å¤‡é€‰: ä½¿ç”¨ httpx ç›´æ¥è¯·æ±‚ DuckDuckGo HTML API
        if HTTPX_AVAILABLE:
            return self._search_with_httpx(query, max_results)

        raise RuntimeError("No search backend available. Install duckduckgo-search or httpx.")

    def _search_with_httpx(self, query: str, max_results: int) -> list:
        """ä½¿ç”¨ httpx æœç´¢ DuckDuckGo HTML API

        è¿™æ˜¯ DDGS å¤±è´¥æ—¶çš„å¤‡é€‰æ–¹æ¡ˆï¼Œç»•è¿‡ primp çš„ SSL é—®é¢˜
        """
        try:
            # DuckDuckGo HTML æœç´¢ URL
            url = f"https://html.duckduckgo.com/html/?q={quote_plus(query)}"

            headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            }

            # ç¦ç”¨ SSL éªŒè¯ä»¥è§£å†³æŸäº›ç¯å¢ƒçš„è¯ä¹¦é—®é¢˜ (å¼€å‘ç¯å¢ƒ)
            # å¢åŠ è¶…æ—¶æ—¶é—´ä»¥å¤„ç†æ…¢é€Ÿç½‘ç»œ
            timeout = httpx.Timeout(60.0, connect=30.0, read=60.0)
            with httpx.Client(timeout=timeout, follow_redirects=True, verify=False) as client:
                response = client.get(url, headers=headers)
                response.raise_for_status()
                html = response.text

            # è§£æ HTML ç»“æœ
            results = []

            # åŒ¹é…æœç´¢ç»“æœå—
            result_pattern = re.compile(
                r'<a[^>]+class="result__a"[^>]+href="([^"]+)"[^>]*>([^<]+)</a>'
                r'.*?<a[^>]+class="result__snippet"[^>]*>([^<]*)</a>',
                re.DOTALL
            )


            # å°è¯•ä¸»æ¨¡å¼
            for match in result_pattern.finditer(html):
                if len(results) >= max_results:
                    break
                link, title, snippet = match.groups()
                # æ¸…ç† HTML å®ä½“
                title = re.sub(r'<[^>]+>', '', title).strip()
                snippet = re.sub(r'<[^>]+>', '', snippet).strip()
                if link and title:
                    results.append({
                        "title": title,
                        "link": link,
                        "snippet": snippet
                    })

            # å¦‚æœä¸»æ¨¡å¼æ²¡æœ‰ç»“æœï¼Œå°è¯•å¤‡ç”¨æ¨¡å¼
            if not results:
                # æ›´ç®€å•çš„æ¨¡å¼: æå–æ‰€æœ‰é“¾æ¥
                link_pattern = re.compile(
                    r'<a[^>]+class="[^"]*result__url[^"]*"[^>]+href="([^"]+)"[^>]*>',
                    re.IGNORECASE
                )
                title_pattern = re.compile(
                    r'<a[^>]+class="[^"]*result__a[^"]*"[^>]*>([^<]+)</a>',
                    re.IGNORECASE
                )
                snippet_pattern = re.compile(
                    r'<a[^>]+class="[^"]*result__snippet[^"]*"[^>]*>([^<]+)</a>',
                    re.IGNORECASE
                )

                links = link_pattern.findall(html)
                titles = title_pattern.findall(html)
                snippets = snippet_pattern.findall(html)

                for i in range(min(len(titles), max_results)):
                    results.append({
                        "title": titles[i] if i < len(titles) else "",
                        "link": links[i] if i < len(links) else "",
                        "snippet": snippets[i] if i < len(snippets) else ""
                    })

            logger.info(f"httpx fallback found {len(results)} results")
            return results

        except Exception as e:
            logger.error(f"httpx search failed: {e}")
            raise

    def format_result(self, result: dict[str, Any]) -> str:
        """æ ¼å¼åŒ–æœç´¢ç»“æœä¸ºå¯è¯»æ–‡æœ¬

        Args:
            result: execute() è¿”å›çš„ç»“æœ

        Returns:
            str: æ ¼å¼åŒ–çš„æ–‡æœ¬
        """
        if not result.get("success"):
            error = result.get("error", "Unknown error")
            return f"âŒ Search failed: {error}"

        results = result.get("results", [])
        query = result.get("query", "")
        count = result.get("count", 0)

        if count == 0:
            return f"ğŸ” No results found for: '{query}'"

        # æ ¼å¼åŒ–æ¯ä¸ªç»“æœ
        formatted = f"ğŸ” Found {count} results for: '{query}'\n\n"

        for i, item in enumerate(results, 1):
            title = item.get("title", "No title")
            link = item.get("link", "")
            snippet = item.get("snippet", "No snippet")

            formatted += f"{i}. **{title}**\n"
            formatted += f"   {link}\n"
            formatted += f"   {snippet[:200]}{'...' if len(snippet) > 200 else ''}\n\n"

        return formatted.strip()


# ä¾¿æ·å‡½æ•°
def create_web_search_tool() -> WebSearchTool:
    """åˆ›å»º web_search å·¥å…·å®ä¾‹

    Returns:
        WebSearchTool: æœç´¢å·¥å…·å®ä¾‹
    """
    return WebSearchTool()
