# -*- coding: utf-8 -*-
"""
DeepResearchAgent - æ·±åº¦ç ”ç©¶ Agent

å®ç° Manus çº§åˆ«çš„æ·±åº¦ç ”ç©¶èƒ½åŠ›ï¼š
- å¤šè½®æœç´¢ä¸æŸ¥è¯¢æ‰©å±• (QueryExpansion)
- æ¥æºå¯ä¿¡åº¦è¯„ä¼° (SourceCredibility)  
- ä¿¡æ¯ç»¼åˆä¸å†²çªè§£å†³ (InformationSynthesis)
- ç»“æ„åŒ–æŠ¥å‘Šç”Ÿæˆ (å¸¦å¼•ç”¨)
- æ—¶å…‰é•¿å»Š (ç ”ç©¶è½¨è¿¹æˆªå›¾å›æº¯)

è®¾è®¡åŸåˆ™ï¼š
- å¤§æ¨¡å‹åœ¨"å®è§‚é€»è¾‘"ä¸Š60%æˆåŠŸç‡ï¼Œåœ¨"å¾®è§‚åŠ¨ä½œ"ä¸Š99.9%æˆåŠŸç‡
- æŠŠ1ä¸ª60%æˆåŠŸç‡çš„å¤§ä»»åŠ¡åˆ‡ç¢æˆ100ä¸ª99.9%æˆåŠŸç‡çš„å°ä»»åŠ¡
"""
from typing import AsyncGenerator, List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
import logging
import json
import re
import asyncio

from ..base import BaseAgent
from ..types import SSEEvent, SSEEventType, AgentAction, ActionType

logger = logging.getLogger(__name__)

# å¹¶å‘é…ç½®
MAX_CONCURRENT_TOOLS = 10  # æœ€å¤§å¹¶å‘å·¥å…·æ‰§è¡Œæ•°


# ==================== æ•°æ®æ¨¡å‹ ====================

class SourceCredibility(Enum):
    """æ¥æºå¯ä¿¡åº¦ç­‰çº§"""
    AUTHORITATIVE = "authoritative"      # æƒå¨æ¥æº (å­¦æœ¯æœŸåˆŠã€å®˜æ–¹æ–‡æ¡£)
    RELIABLE = "reliable"                 # å¯é æ¥æº (çŸ¥ååª’ä½“ã€ä¸“ä¸šåšå®¢)
    MODERATE = "moderate"                 # ä¸€èˆ¬æ¥æº (æ™®é€šç½‘ç«™)
    QUESTIONABLE = "questionable"         # å¯ç–‘æ¥æº (æœªçŸ¥æ¥æº)
    UNRELIABLE = "unreliable"            # ä¸å¯é æ¥æº (å·²çŸ¥å‡ä¿¡æ¯æº)


@dataclass
class ResearchSource:
    """ç ”ç©¶æ¥æº"""
    url: str
    title: str
    snippet: str
    credibility: SourceCredibility = SourceCredibility.MODERATE
    timestamp: datetime = field(default_factory=datetime.now)
    content: Optional[str] = None        # å®Œæ•´å†…å®¹ (read_url åå¡«å……)
    screenshot_path: Optional[str] = None  # æˆªå›¾è·¯å¾„ (æ—¶å…‰é•¿å»Š)
    key_findings: List[str] = field(default_factory=list)
    
    def to_citation(self, index: int) -> str:
        """ç”Ÿæˆå¼•ç”¨æ ¼å¼"""
        date_str = self.timestamp.strftime("%Y-%m-%d")
        return f"[{index}] {self.title}. {self.url}. Accessed: {date_str}"


@dataclass
class ResearchFinding:
    """ç ”ç©¶å‘ç°"""
    content: str
    sources: List[ResearchSource]
    confidence: float  # 0.0 - 1.0
    conflicting_sources: List[ResearchSource] = field(default_factory=list)


@dataclass
class ResearchState:
    """ç ”ç©¶çŠ¶æ€ (Working Memory)"""
    topic: str
    queries_executed: List[str] = field(default_factory=list)
    sources_collected: List[ResearchSource] = field(default_factory=list)
    findings: List[ResearchFinding] = field(default_factory=list)
    knowledge_gaps: List[str] = field(default_factory=list)
    phase: str = "init"  # init -> searching -> reading -> synthesizing -> reporting
    iteration: int = 0
    max_sources: int = 10
    min_credible_sources: int = 3


# ==================== æ ¸å¿ƒ Agent ====================

class DeepResearchAgent(BaseAgent):
    """æ·±åº¦ç ”ç©¶ Agent
    
    ç»§æ‰¿ ResearchAgentï¼Œæ‰©å±•ä»¥ä¸‹èƒ½åŠ›ï¼š
    - å¤šè½®æœç´¢ï¼šåŸºäºåˆå§‹ç»“æœæ‰©å±•æŸ¥è¯¢
    - æ¥æºè¯„ä¼°ï¼šè‡ªåŠ¨è¯„ä¼°æ¥æºå¯ä¿¡åº¦
    - ä¿¡æ¯ç»¼åˆï¼šè¯†åˆ«å…±è¯†ä¸å†²çª
    - æŠ¥å‘Šç”Ÿæˆï¼šå¸¦å¼•ç”¨çš„ç»“æ„åŒ–æŠ¥å‘Š
    
    å·¥ä½œæµï¼š
    1. ç†è§£ç ”ç©¶ä¸»é¢˜ â†’ ç”Ÿæˆåˆå§‹æŸ¥è¯¢
    2. æ‰§è¡Œæœç´¢ â†’ æ”¶é›†æ¥æº
    3. è¯„ä¼°æ¥æº â†’ ç­›é€‰é«˜è´¨é‡æ¥æº
    4. æ·±åº¦é˜…è¯» â†’ æå–å…³é”®å‘ç°
    5. æ‰©å±•æŸ¥è¯¢ â†’ å¡«è¡¥çŸ¥è¯†ç©ºç™½
    6. ä¿¡æ¯ç»¼åˆ â†’ ç”ŸæˆæŠ¥å‘Š
    """
    
    def __init__(self, *args, max_concurrent: int = MAX_CONCURRENT_TOOLS, **kwargs):
        super().__init__(*args, **kwargs)
        self.research_state: Optional[ResearchState] = None
        self._action_count = 0  # ç”¨äº 2-Action Rule
        self._max_concurrent = max_concurrent  # æœ€å¤§å¹¶å‘æ•°
        self._semaphore = asyncio.Semaphore(max_concurrent)  # å¹¶å‘æ§åˆ¶
        self._pending_urls: List[str] = []  # å¾…å¹¶å‘è¯»å–çš„ URL
        self._pending_queries: List[str] = []  # å¾…å¹¶å‘æœç´¢çš„æŸ¥è¯¢
    
    async def _think(self) -> AsyncGenerator[SSEEvent, None]:
        """æ€è€ƒè¿‡ç¨‹ - DeepResearch ç‰ˆæœ¬
        
        æ ¹æ®å½“å‰ç ”ç©¶é˜¶æ®µè¿›è¡Œé’ˆå¯¹æ€§æ€è€ƒ
        """
        logger.debug("DeepResearchAgent thinking...")
        
        phase = self.research_state.phase if self.research_state else "init"
        
        # é˜¶æ®µæ€§æ€è€ƒæç¤º
        thinking_prompts = {
            "init": "ğŸ¯ Analyzing research topic and planning initial search strategy...",
            "searching": "ğŸ” Evaluating search results and identifying knowledge gaps...",
            "reading": "ğŸ“– Extracting key findings and assessing source credibility...",
            "synthesizing": "ğŸ”— Synthesizing information and resolving conflicts...",
            "reporting": "ğŸ“ Generating structured research report..."
        }
        
        yield SSEEvent(
            type=SSEEventType.THINKING,
            data={'content': thinking_prompts.get(phase, "Thinking...") + "\n"}
        )
        
        # æ„é€ æ€è€ƒç³»ç»Ÿæç¤º
        system_prompt = self._get_thinking_prompt(phase)
        
        # ä½¿ç”¨ LLM è¿›è¡Œæ€è€ƒ
        thinking_content = ""
        async for chunk in self.llm.stream(
            messages=self.context.messages,
            system=system_prompt
        ):
            thinking_content += chunk
            yield SSEEvent(
                type=SSEEventType.THINKING,
                data={'content': chunk}
            )
        
        # ä¿å­˜æ€è€ƒå†…å®¹
        self.context.append_thinking(thinking_content)
        
        # è§£ææ€è€ƒç»“æœï¼Œæ›´æ–°ç ”ç©¶çŠ¶æ€
        await self._update_state_from_thinking(thinking_content)
        
        logger.debug(f"Thinking complete, phase: {phase}")
    
    def _get_thinking_prompt(self, phase: str) -> str:
        """è·å–é˜¶æ®µæ€§æ€è€ƒæç¤º"""
        
        base_prompt = """You are a deep research assistant conducting systematic research.

Current Research State:
"""
        if self.research_state:
            base_prompt += f"""- Topic: {self.research_state.topic}
- Phase: {phase}
- Sources collected: {len(self.research_state.sources_collected)}
- Queries executed: {len(self.research_state.queries_executed)}
- Knowledge gaps: {self.research_state.knowledge_gaps}
"""
        
        phase_prompts = {
            "init": """
Analyze the research topic and:
1. Break down the topic into key aspects to investigate
2. Generate 3-5 specific search queries
3. Identify potential authoritative sources

Output your thinking as structured analysis.""",

            "searching": """
Evaluate the current search results:
1. Assess source credibility (authoritative, reliable, moderate, questionable)
2. Identify knowledge gaps - what aspects are not yet covered?
3. Generate follow-up queries to fill gaps

Focus on finding diverse, high-quality sources.""",

            "reading": """
For each source being read:
1. Extract key findings and claims
2. Note any data, statistics, or quotes
3. Identify conflicting information
4. Rate information completeness

Be thorough but concise.""",

            "synthesizing": """
Synthesize all collected information:
1. Identify consensus across sources
2. Note conflicting viewpoints and their sources
3. Assess overall confidence in findings
4. Structure the main conclusions

Prepare for report generation.""",

            "reporting": """
Plan the research report structure:
1. Executive summary
2. Key findings by aspect
3. Conflicting viewpoints
4. Limitations and gaps
5. References

Ensure all claims have citations."""
        }
        
        return base_prompt + phase_prompts.get(phase, "Analyze the current situation.")
    
    async def _update_state_from_thinking(self, thinking: str) -> None:
        """ä»æ€è€ƒå†…å®¹æ›´æ–°ç ”ç©¶çŠ¶æ€"""
        if not self.research_state:
            return
        
        # ç®€å•çš„çŠ¶æ€è½¬æ¢é€»è¾‘ï¼ˆå¯ä»¥ç”¨ LLM è§£ææ›´å¤æ‚çš„å†…å®¹ï¼‰
        if self.research_state.phase == "init":
            # æå–æŸ¥è¯¢å»ºè®®
            queries = self._extract_queries_from_thinking(thinking)
            if queries:
                self.research_state.knowledge_gaps = queries
        
        elif self.research_state.phase == "searching":
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›å…¥ä¸‹ä¸€é˜¶æ®µ
            if len(self.research_state.sources_collected) >= self.research_state.min_credible_sources:
                # æœ‰è¶³å¤Ÿæ¥æºï¼Œå¯ä»¥å¼€å§‹æ·±åº¦é˜…è¯»
                pass
    
    def _extract_queries_from_thinking(self, thinking: str) -> List[str]:
        """ä»æ€è€ƒå†…å®¹æå–æŸ¥è¯¢å»ºè®®"""
        queries = []
        # ç®€å•æ­£åˆ™åŒ¹é…æ•°å­—åˆ—è¡¨æ ¼å¼
        pattern = r'(?:^|\n)\s*\d+[\.\)]\s*(.+?)(?=\n\s*\d+[\.\)]|\n\n|$)'
        matches = re.findall(pattern, thinking, re.MULTILINE)
        for match in matches[:5]:  # æœ€å¤š5ä¸ª
            query = match.strip().strip('"\'')
            if len(query) > 10 and len(query) < 200:
                queries.append(query)
        return queries
    
    async def _decide(self) -> AgentAction:
        """å†³ç­– - DeepResearch ç‰ˆæœ¬
        
        æ ¹æ®ç ”ç©¶é˜¶æ®µå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨
        """
        logger.debug(f"DeepResearchAgent deciding, phase: {self.research_state.phase if self.research_state else 'init'}")
        
        # åˆå§‹åŒ–ç ”ç©¶çŠ¶æ€
        if not self.research_state:
            topic = self._extract_topic_from_context()
            self.research_state = ResearchState(topic=topic)
            logger.info(f"Research initialized for topic: {topic}")
        
        # è·å–å¯ç”¨å·¥å…·
        tool_definitions = self.tools.get_llm_tool_definitions()
        
        if not tool_definitions:
            logger.warning("No tools available, generating answer directly")
            return await self._generate_final_report()
        
        # æ ¹æ®é˜¶æ®µå†³å®šç³»ç»Ÿæç¤º
        system_prompt = self._get_decision_prompt()
        
        # è°ƒç”¨ LLM è¿›è¡Œå†³ç­–
        response = await self.llm.complete(
            messages=self.context.messages,
            system=system_prompt,
            tools=tool_definitions
        )
        
        # å¤„ç†å·¥å…·è°ƒç”¨
        if response.tool_calls:
            tool_call = response.tool_calls[0]
            
            # æ›´æ–° action count (2-Action Rule)
            self._action_count += 1
            
            # æ¯2æ¬¡é‡å¤§æ“ä½œåï¼Œè§¦å‘å†™å…¥ findings
            if self._action_count >= 2 and tool_call["name"] in ["web_search", "read_url", "browser_open"]:
                self._action_count = 0
                # è®°å½•åˆ° findings.md
                await self._record_findings()
            
            # æ›´æ–°é˜¶æ®µ
            self._update_phase_from_tool(tool_call["name"])
            
            return AgentAction(
                type=ActionType.TOOL_CALL,
                tool_name=tool_call["name"],
                tool_input=tool_call["input"],
                tool_call_id=tool_call["id"]
            )
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç”ŸæˆæŠ¥å‘Š
        if self._should_generate_report():
            return await self._generate_final_report()
        
        # é»˜è®¤ç»§ç»­ç ”ç©¶
        answer = response.content.strip()
        return AgentAction(
            type=ActionType.ANSWER,
            answer=answer
        )
    
    def _get_decision_prompt(self) -> str:
        """è·å–å†³ç­–æç¤º"""
        phase = self.research_state.phase if self.research_state else "init"
        
        base = """You are conducting deep research. Based on the conversation and your analysis:

Available tools:
- web_search: Search for information (use for broad queries)
- read_url: Read full content from a URL (use for deep reading)
- browser_open: Open page with interactive elements (use for dynamic pages)
- browser_screenshot: Capture page screenshot (use for timeline)

Current research state:
"""
        if self.research_state:
            base += f"""- Topic: {self.research_state.topic}
- Phase: {phase}
- Sources: {len(self.research_state.sources_collected)}/{self.research_state.max_sources}
- Queries done: {self.research_state.queries_executed}
"""
        
        phase_instructions = {
            "init": """
You are starting research. Your first action should be:
1. Use web_search with a well-crafted query about the topic
2. Focus on finding authoritative sources first

Do NOT generate a final answer yet - gather information first.""",

            "searching": """
Continue gathering sources:
1. If you have < 3 credible sources, continue searching with refined queries
2. If you found good sources, use read_url to get full content
3. Consider different perspectives on the topic

Prioritize quality over quantity.""",

            "reading": """
You are in deep reading phase:
1. Use read_url to get full content from promising sources
2. Use browser_screenshot to capture important pages for timeline
3. Once you have enough content, move to synthesis

Extract specific facts, quotes, and data.""",

            "synthesizing": """
You have gathered enough information. Now:
1. If any critical gaps remain, do one more targeted search
2. Otherwise, generate a comprehensive research report

Your report should include:
- Executive Summary
- Key Findings (with citations)
- Conflicting Viewpoints
- Limitations
- References""",

            "reporting": """
Generate the final research report now.
Include all citations and organize findings clearly.
Do NOT call any more tools - provide the complete answer."""
        }
        
        return base + phase_instructions.get(phase, "Continue research based on current needs.")
    
    def _update_phase_from_tool(self, tool_name: str) -> None:
        """æ ¹æ®å·¥å…·è°ƒç”¨æ›´æ–°é˜¶æ®µ"""
        if not self.research_state:
            return
        
        if tool_name == "web_search":
            if self.research_state.phase == "init":
                self.research_state.phase = "searching"
        
        elif tool_name in ["read_url", "browser_open"]:
            if self.research_state.phase in ["init", "searching"]:
                self.research_state.phase = "reading"
        
        elif tool_name == "browser_screenshot":
            # æˆªå›¾ä¸æ”¹å˜é˜¶æ®µ
            pass
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿›å…¥ç»¼åˆé˜¶æ®µ
        if len(self.research_state.sources_collected) >= self.research_state.min_credible_sources:
            if self.research_state.phase == "reading":
                self.research_state.phase = "synthesizing"
    
    def _should_generate_report(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç”ŸæˆæŠ¥å‘Š"""
        if not self.research_state:
            return False
        
        # å·²ç»åœ¨æŠ¥å‘Šé˜¶æ®µ
        if self.research_state.phase == "reporting":
            return True
        
        # è¾¾åˆ°æœ€å¤§æ¥æºæ•°
        if len(self.research_state.sources_collected) >= self.research_state.max_sources:
            self.research_state.phase = "reporting"
            return True
        
        # è¿­ä»£æ¬¡æ•°è¿‡å¤š
        if self.research_state.iteration > 15:
            self.research_state.phase = "reporting"
            return True
        
        return False
    
    async def _generate_final_report(self) -> AgentAction:
        """ç”Ÿæˆæœ€ç»ˆç ”ç©¶æŠ¥å‘Š"""
        logger.info("Generating final research report...")
        
        # æ„é€ æŠ¥å‘Šç”Ÿæˆæç¤º
        system_prompt = """Generate a comprehensive research report based on all gathered information.

Structure:
1. **Executive Summary** (2-3 sentences)
2. **Key Findings** (numbered list with source citations [1], [2], etc.)
3. **Analysis** (synthesize the findings)
4. **Conflicting Viewpoints** (if any)
5. **Limitations & Gaps** (what couldn't be determined)
6. **References** (numbered list of all sources)

Use markdown formatting. Be thorough but concise.
Every factual claim MUST have a citation."""

        # æ·»åŠ æ¥æºä¿¡æ¯åˆ°ä¸Šä¸‹æ–‡
        sources_context = self._format_sources_for_report()
        
        messages = self.context.messages.copy()
        messages.append({
            "role": "user",
            "content": f"Based on the research, generate the final report.\n\nCollected Sources:\n{sources_context}"
        })
        
        response = await self.llm.complete(
            messages=messages,
            system=system_prompt
        )
        
        report = response.content.strip()
        
        # æ·»åŠ å¼•ç”¨éƒ¨åˆ†
        if self.research_state and self.research_state.sources_collected:
            report += "\n\n---\n\n## References\n\n"
            for i, source in enumerate(self.research_state.sources_collected, 1):
                report += source.to_citation(i) + "\n"
        
        # è®°å½•åˆ° findings.md
        await self.memory.write_findings(
            "Research Report Generated",
            report[:2000]  # æ‘˜è¦
        )
        
        self.research_state.phase = "reporting"
        
        return AgentAction(
            type=ActionType.ANSWER,
            answer=report
        )
    
    def _format_sources_for_report(self) -> str:
        """æ ¼å¼åŒ–æ¥æºä¿¡æ¯ç”¨äºæŠ¥å‘Šç”Ÿæˆ"""
        if not self.research_state or not self.research_state.sources_collected:
            return "No sources collected yet."
        
        formatted = ""
        for i, source in enumerate(self.research_state.sources_collected, 1):
            formatted += f"\n[{i}] {source.title}\n"
            formatted += f"    URL: {source.url}\n"
            formatted += f"    Credibility: {source.credibility.value}\n"
            if source.key_findings:
                formatted += f"    Key Findings:\n"
                for finding in source.key_findings:
                    formatted += f"      - {finding}\n"
            if source.snippet:
                formatted += f"    Snippet: {source.snippet[:300]}...\n"
        
        return formatted
    
    async def _record_findings(self) -> None:
        """è®°å½•å‘ç°åˆ° findings.md (2-Action Rule)"""
        if not self.research_state:
            return
        
        summary = f"Research Progress - Phase: {self.research_state.phase}\n"
        summary += f"Sources: {len(self.research_state.sources_collected)}\n"
        summary += f"Queries: {self.research_state.queries_executed[-3:]}\n"  # æœ€è¿‘3ä¸ªæŸ¥è¯¢
        
        try:
            await self.memory.write_findings(
                f"Research Update ({self.research_state.topic[:50]})",
                summary
            )
        except Exception as e:
            logger.warning(f"Failed to record findings: {e}")
    
    def _extract_topic_from_context(self) -> str:
        """ä»ä¸Šä¸‹æ–‡æå–ç ”ç©¶ä¸»é¢˜"""
        if not self.context.messages:
            return "Unknown topic"
        
        # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        for msg in reversed(self.context.messages):
            if msg.get("role") == "user":
                content = msg.get("content", "")
                # æ¸…ç†å†…å®¹ï¼Œæå–ä¸»è¦ä¸»é¢˜
                topic = content.strip()[:200]
                return topic
        
        return "Unknown topic"
    
    # ==================== æ¥æºå¯ä¿¡åº¦è¯„ä¼° ====================
    
    def assess_source_credibility(self, url: str, title: str) -> SourceCredibility:
        """è¯„ä¼°æ¥æºå¯ä¿¡åº¦
        
        åŸºäº URL åŸŸåå’Œæ ‡é¢˜è¿›è¡Œåˆæ­¥è¯„ä¼°
        """
        url_lower = url.lower()
        
        # æƒå¨æ¥æºæ¨¡å¼
        authoritative_patterns = [
            ".gov", ".edu", "arxiv.org", "nature.com", "science.org",
            "ieee.org", "acm.org", "springer.com", "wiley.com",
            "nih.gov", "who.int", "un.org"
        ]
        
        # å¯é æ¥æºæ¨¡å¼
        reliable_patterns = [
            "github.com", "stackoverflow.com", "medium.com",
            "nytimes.com", "bbc.com", "reuters.com", "theguardian.com",
            "techcrunch.com", "wired.com", "arstechnica.com",
            "docs.python.org", "docs.microsoft.com", "developer.mozilla.org"
        ]
        
        # æ£€æŸ¥æƒå¨æ¥æº
        for pattern in authoritative_patterns:
            if pattern in url_lower:
                return SourceCredibility.AUTHORITATIVE
        
        # æ£€æŸ¥å¯é æ¥æº
        for pattern in reliable_patterns:
            if pattern in url_lower:
                return SourceCredibility.RELIABLE
        
        # é»˜è®¤ä¸ºä¸€èˆ¬å¯ä¿¡åº¦
        return SourceCredibility.MODERATE
    
    def add_source(self, url: str, title: str, snippet: str) -> ResearchSource:
        """æ·»åŠ ç ”ç©¶æ¥æº"""
        credibility = self.assess_source_credibility(url, title)
        
        source = ResearchSource(
            url=url,
            title=title,
            snippet=snippet,
            credibility=credibility
        )
        
        if self.research_state:
            self.research_state.sources_collected.append(source)
            logger.info(f"Added source: {title} (credibility: {credibility.value})")
        
        return source
    
    # ==================== å¹¶å‘æ‰§è¡Œæ”¯æŒ ====================
    
    async def _execute_tool_with_semaphore(
        self, 
        tool_name: str, 
        tool_input: Dict[str, Any]
    ) -> Tuple[str, Dict[str, Any]]:
        """å¸¦ä¿¡å·é‡æ§åˆ¶çš„å·¥å…·æ‰§è¡Œ
        
        Args:
            tool_name: å·¥å…·åç§°
            tool_input: å·¥å…·è¾“å…¥å‚æ•°
            
        Returns:
            Tuple[str, Dict]: (å·¥å…·å, æ‰§è¡Œç»“æœ)
        """
        async with self._semaphore:
            try:
                tool = self.tools.get(tool_name)
                if not tool:
                    return (tool_name, {"success": False, "error": f"Tool {tool_name} not found"})
                
                result = await tool.execute(**tool_input)
                return (tool_name, result)
            except Exception as e:
                logger.error(f"Tool {tool_name} execution failed: {e}")
                return (tool_name, {"success": False, "error": str(e)})
    
    async def execute_tools_concurrently(
        self,
        tool_calls: List[Tuple[str, Dict[str, Any]]]
    ) -> AsyncGenerator[SSEEvent, None]:
        """å¹¶å‘æ‰§è¡Œå¤šä¸ªå·¥å…·
        
        Args:
            tool_calls: List of (tool_name, tool_input) tuples
            
        Yields:
            SSEEvent: æ‰§è¡Œè¿›åº¦å’Œç»“æœäº‹ä»¶
        """
        if not tool_calls:
            return
        
        # é™åˆ¶å¹¶å‘æ•°
        batch_size = min(len(tool_calls), self._max_concurrent)
        
        yield SSEEvent(
            type=SSEEventType.STATUS,
            data={
                'content': f'ğŸš€ Executing {len(tool_calls)} tools concurrently (max {batch_size} parallel)...\n'
            }
        )
        
        # åˆ›å»ºä»»åŠ¡
        tasks = [
            self._execute_tool_with_semaphore(name, inputs)
            for name, inputs in tool_calls
        ]
        
        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        successful = 0
        failed = 0
        
        for i, result in enumerate(results):
            tool_name, tool_input = tool_calls[i]
            
            if isinstance(result, Exception):
                failed += 1
                yield SSEEvent(
                    type=SSEEventType.TOOL_ERROR,
                    data={
                        'tool': tool_name,
                        'error': str(result)
                    }
                )
            else:
                name, output = result
                if output.get("success", False):
                    successful += 1
                    # å¤„ç†æœç´¢ç»“æœ - æ”¶é›† URL
                    if name == "web_search" and output.get("results"):
                        for item in output.get("results", []):
                            url = item.get("link") or item.get("url")
                            if url:
                                self._pending_urls.append(url)
                    
                    # å¤„ç† read_url ç»“æœ - æ·»åŠ æ¥æº
                    if name == "read_url" and output.get("content"):
                        self.add_source(
                            url=output.get("url", ""),
                            title=output.get("title", "Unknown"),
                            snippet=output.get("content", "")[:500]
                        )
                else:
                    failed += 1
                
                yield SSEEvent(
                    type=SSEEventType.TOOL_RESULT,
                    data={
                        'tool': name,
                        'result': output,
                        'success': output.get("success", False)
                    }
                )
        
        yield SSEEvent(
            type=SSEEventType.STATUS,
            data={
                'content': f'âœ… Concurrent execution complete: {successful} succeeded, {failed} failed\n'
            }
        )
        
        # æ›´æ–° action count
        self._action_count += len(tool_calls)
        
        # 2-Action Rule: æ¯ 2 æ¬¡é‡å¤§æ“ä½œåå†™å…¥ findings
        if self._action_count >= 2:
            self._action_count = 0
            await self._record_findings()
    
    async def batch_search(self, queries: List[str]) -> AsyncGenerator[SSEEvent, None]:
        """æ‰¹é‡å¹¶å‘æœç´¢
        
        Args:
            queries: æœç´¢æŸ¥è¯¢åˆ—è¡¨
            
        Yields:
            SSEEvent: æ‰§è¡Œäº‹ä»¶
        """
        tool_calls = [
            ("web_search", {"query": q, "max_results": 5})
            for q in queries[:self._max_concurrent]  # é™åˆ¶æ•°é‡
        ]
        
        if self.research_state:
            self.research_state.queries_executed.extend(queries[:self._max_concurrent])
        
        async for event in self.execute_tools_concurrently(tool_calls):
            yield event
    
    async def batch_read_urls(self, urls: List[str]) -> AsyncGenerator[SSEEvent, None]:
        """æ‰¹é‡å¹¶å‘è¯»å– URL
        
        Args:
            urls: URL åˆ—è¡¨
            
        Yields:
            SSEEvent: æ‰§è¡Œäº‹ä»¶
        """
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_urls = list(dict.fromkeys(urls))[:self._max_concurrent]
        
        tool_calls = [
            ("read_url", {"url": url, "use_jina": True, "max_length": 8000})
            for url in unique_urls
        ]
        
        async for event in self.execute_tools_concurrently(tool_calls):
            yield event
    
    def get_pending_urls(self) -> List[str]:
        """è·å–å¾…è¯»å–çš„ URL å¹¶æ¸…ç©º"""
        urls = self._pending_urls.copy()
        self._pending_urls.clear()
        return urls
    
    def queue_urls_for_reading(self, urls: List[str]) -> None:
        """å°† URL åŠ å…¥å¾…è¯»å–é˜Ÿåˆ—"""
        self._pending_urls.extend(urls)
    
    # ==================== æµå¼è¿”å› (Streaming) ====================
    
    async def execute_tools_streaming(
        self,
        tool_calls: List[Tuple[str, Dict[str, Any]]]
    ) -> AsyncGenerator[SSEEvent, None]:
        """æµå¼æ‰§è¡Œå·¥å…· - ä½¿ç”¨ asyncio.as_completed å®æ—¶è¿”å›
        
        ä¸ execute_tools_concurrently çš„åŒºåˆ«:
        - as_completed: å“ªä¸ªå…ˆå®Œæˆå°±å…ˆè¿”å›ï¼Œç”¨æˆ·æ›´å¿«çœ‹åˆ°ç»“æœ
        - gather: ç­‰æ‰€æœ‰å®Œæˆåä¸€èµ·è¿”å›
        
        Args:
            tool_calls: List of (tool_name, tool_input) tuples
            
        Yields:
            SSEEvent: å®æ—¶æ‰§è¡Œç»“æœ
        """
        if not tool_calls:
            return
        
        yield SSEEvent(
            type=SSEEventType.STATUS,
            data={
                'content': f'ğŸš€ Streaming {len(tool_calls)} tools (results as they complete)...\n'
            }
        )
        
        # åˆ›å»ºå¸¦ç´¢å¼•çš„ä»»åŠ¡
        async def execute_with_index(idx: int, name: str, inputs: Dict) -> Tuple[int, str, Dict]:
            result = await self._execute_tool_with_semaphore(name, inputs)
            return (idx, result[0], result[1])
        
        tasks = [
            asyncio.create_task(execute_with_index(i, name, inputs))
            for i, (name, inputs) in enumerate(tool_calls)
        ]
        
        completed = 0
        successful = 0
        failed = 0
        
        # ä½¿ç”¨ as_completed å®ç°æµå¼è¿”å›
        for coro in asyncio.as_completed(tasks):
            try:
                idx, tool_name, result = await coro
                completed += 1
                
                if isinstance(result, dict) and result.get("success", False):
                    successful += 1
                    
                    # å¤„ç†æœç´¢ç»“æœ
                    if tool_name == "web_search" and result.get("results"):
                        for item in result.get("results", []):
                            url = item.get("link") or item.get("url")
                            if url:
                                self._pending_urls.append(url)
                    
                    # å¤„ç† read_url ç»“æœ
                    if tool_name == "read_url" and result.get("content"):
                        self.add_source(
                            url=result.get("url", ""),
                            title=result.get("title", "Unknown"),
                            snippet=result.get("content", "")[:500]
                        )
                else:
                    failed += 1
                
                # å®æ—¶æ¨é€ç»“æœ
                yield SSEEvent(
                    type=SSEEventType.TOOL_RESULT,
                    data={
                        'tool': tool_name,
                        'result': result,
                        'success': result.get("success", False) if isinstance(result, dict) else False,
                        'progress': f"{completed}/{len(tool_calls)}"
                    }
                )
                
            except Exception as e:
                completed += 1
                failed += 1
                logger.error(f"Streaming task error: {e}")
                yield SSEEvent(
                    type=SSEEventType.TOOL_ERROR,
                    data={'error': str(e), 'progress': f"{completed}/{len(tool_calls)}"}
                )
        
        yield SSEEvent(
            type=SSEEventType.STATUS,
            data={
                'content': f'âœ… Streaming complete: {successful} succeeded, {failed} failed\n'
            }
        )
        
        # æ›´æ–° action count
        self._action_count += len(tool_calls)
        if self._action_count >= 2:
            self._action_count = 0
            await self._record_findings()
    
    async def batch_search_streaming(
        self,
        queries: List[str]
    ) -> AsyncGenerator[SSEEvent, None]:
        """æµå¼æ‰¹é‡æœç´¢ - ç»“æœå®æ—¶è¿”å›
        
        ç”¨æˆ·ä½“éªŒä¼˜åŒ–:
        - é¦–ä¸ªç»“æœå»¶è¿Ÿ: ~2s (ä¹‹å‰éœ€è¦ç­‰æ‰€æœ‰å®Œæˆ: ~10s)
        """
        tool_calls = [
            ("web_search", {"query": q, "max_results": 5})
            for q in queries[:self._max_concurrent]
        ]
        
        if self.research_state:
            self.research_state.queries_executed.extend(queries[:self._max_concurrent])
        
        async for event in self.execute_tools_streaming(tool_calls):
            yield event
    
    async def batch_read_urls_streaming(
        self,
        urls: List[str],
        query: Optional[str] = None
    ) -> AsyncGenerator[SSEEvent, None]:
        """æµå¼æ‰¹é‡è¯»å– URL
        
        Args:
            urls: URL åˆ—è¡¨
            query: ç ”ç©¶æŸ¥è¯¢ (ç”¨äº extract_relevant)
        """
        unique_urls = list(dict.fromkeys(urls))[:self._max_concurrent]
        
        tool_calls = [
            ("read_url", {
                "url": url,
                "use_jina": True,
                "extract_relevant": bool(query),
                "query": query or "",
                "max_length": 8000
            })
            for url in unique_urls
        ]
        
        async for event in self.execute_tools_streaming(tool_calls):
            yield event


# ==================== å·¥å‚å‡½æ•° ====================

async def create_deep_research_agent(
    context,
    llm,
    tools,
    memory,
    db,
    max_iterations: int = 30,
    max_sources: int = 10,
    max_concurrent: int = MAX_CONCURRENT_TOOLS
) -> DeepResearchAgent:
    """åˆ›å»º DeepResearchAgent å®ä¾‹
    
    Args:
        context: AgentContext
        llm: BaseLLM
        tools: ToolRegistry
        memory: WorkingMemory
        db: AsyncSession
        max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆæ·±åº¦ç ”ç©¶éœ€è¦æ›´å¤šè¿­ä»£ï¼‰
        max_sources: æœ€å¤§æ¥æºæ•°
        max_concurrent: æœ€å¤§å¹¶å‘å·¥å…·æ‰§è¡Œæ•° (é»˜è®¤ 10)
        
    Returns:
        DeepResearchAgent: Agent å®ä¾‹
    """
    agent = DeepResearchAgent(
        context=context,
        llm=llm,
        tools=tools,
        memory=memory,
        db=db,
        max_iterations=max_iterations,
        max_concurrent=max_concurrent
    )
    
    # åˆå§‹åŒ–ç ”ç©¶çŠ¶æ€å‚æ•°
    if agent.research_state:
        agent.research_state.max_sources = max_sources
    
    logger.info(
        f"DeepResearchAgent created with max_iterations={max_iterations}, "
        f"max_sources={max_sources}, max_concurrent={max_concurrent}"
    )
    return agent
