"""
DeepResearchAgent - æ·±åº¦ç ”ç©¶ Agent

å®ç° Manus çº§åˆ«çš„æ·±åº¦ç ”ç©¶èƒ½åŠ›ï¼š
- å¤šè½®æœç´¢ä¸æŸ¥è¯¢æ‰©å±• (QueryExpansion)
- æ¥æºå¯ä¿¡åº¦è¯„ä¼° (SourceCredibility)
- ä¿¡æ¯ç»¼åˆä¸å†²çªè§£å†³ (InformationSynthesis)
- ç»“æ„åŒ–æŠ¥å‘Šç”Ÿæˆ (å¸¦å¼•ç”¨)
- æ—¶å…‰é•¿å»Š (ç ”ç©¶è½¨è¿¹æˆªå›¾å›æº¯)

è®¾è®¡åŸåˆ™ï¼š
- å¤§æ¨¡å‹åœ¨"å®è§‚é€»è¾‘"ä¸Š60%æˆåŠŸç‡ï¼Œåœ¨"å¾®è§‚åŠ¨ä½œ"ä¸Š99.9%æˆåŠŸç‡
- æŠŠ1ä¸ª60%æˆåŠŸç‡çš„å¤§ä»»åŠ¡åˆ‡ç¢æˆ100ä¸ª99.9%æˆåŠŸç‡çš„å°ä»»åŠ¡
"""
import asyncio
import logging
import os
import re
from collections.abc import AsyncGenerator
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any

from ..base import BaseAgent
from ..types import ActionType, AgentAction, Citation, CitationSource, SSEEvent, SSEEventType

logger = logging.getLogger(__name__)

# å¹¶å‘é…ç½®: ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œé»˜è®¤ 15
MAX_CONCURRENT_TOOLS = int(os.getenv("MAX_CONCURRENT_TOOLS", "15"))


# ==================== æ•°æ®æ¨¡å‹ ====================

class SourceCredibility(Enum):
    """æ¥æºå¯ä¿¡åº¦ç­‰çº§"""
    AUTHORITATIVE = "authoritative"      # æƒå¨æ¥æº (å­¦æœ¯æœŸåˆŠã€å®˜æ–¹æ–‡æ¡£)
    RELIABLE = "reliable"                 # å¯é æ¥æº (çŸ¥ååª’ä½“ã€ä¸“ä¸šåšå®¢)
    MODERATE = "moderate"                 # ä¸€èˆ¬æ¥æº (æ™®é€šç½‘ç«™)
    QUESTIONABLE = "questionable"         # å¯ç–‘æ¥æº (æœªçŸ¥æ¥æº)
    UNRELIABLE = "unreliable"            # ä¸å¯é æ¥æº (å·²çŸ¥å‡ä¿¡æ¯æº)


@dataclass
class ResearchSource:
    """ç ”ç©¶æ¥æº"""
    url: str
    title: str
    snippet: str
    credibility: SourceCredibility = SourceCredibility.MODERATE
    timestamp: datetime = field(default_factory=datetime.now)
    content: str | None = None        # å®Œæ•´å†…å®¹ (read_url åå¡«å……)
    screenshot_path: str | None = None  # æˆªå›¾è·¯å¾„ (æ—¶å…‰é•¿å»Š)
    key_findings: list[str] = field(default_factory=list)

    def to_citation(self, index: int) -> str:
        """ç”Ÿæˆå¼•ç”¨æ ¼å¼"""
        date_str = self.timestamp.strftime("%Y-%m-%d")
        return f"[{index}] {self.title}. {self.url}. Accessed: {date_str}"


@dataclass
class ResearchFinding:
    """ç ”ç©¶å‘ç°"""
    content: str
    sources: list[ResearchSource]
    confidence: float  # 0.0 - 1.0
    conflicting_sources: list[ResearchSource] = field(default_factory=list)


@dataclass
class ResearchState:
    """ç ”ç©¶çŠ¶æ€ (Working Memory)"""
    topic: str
    queries_executed: list[str] = field(default_factory=list)
    sources_collected: list[ResearchSource] = field(default_factory=list)
    findings: list[ResearchFinding] = field(default_factory=list)
    knowledge_gaps: list[str] = field(default_factory=list)
    phase: str = "init"  # init -> searching -> reading -> synthesizing -> reporting
    iteration: int = 0
    max_sources: int = 10
    min_credible_sources: int = 3


# ==================== æ ¸å¿ƒ Agent ====================

class DeepResearchAgent(BaseAgent):
    """æ·±åº¦ç ”ç©¶ Agent

    ç»§æ‰¿ ResearchAgentï¼Œæ‰©å±•ä»¥ä¸‹èƒ½åŠ›ï¼š
    - å¤šè½®æœç´¢ï¼šåŸºäºåˆå§‹ç»“æœæ‰©å±•æŸ¥è¯¢
    - æ¥æºè¯„ä¼°ï¼šè‡ªåŠ¨è¯„ä¼°æ¥æºå¯ä¿¡åº¦
    - ä¿¡æ¯ç»¼åˆï¼šè¯†åˆ«å…±è¯†ä¸å†²çª
    - æŠ¥å‘Šç”Ÿæˆï¼šå¸¦å¼•ç”¨çš„ç»“æ„åŒ–æŠ¥å‘Š

    å·¥ä½œæµï¼š
    1. ç†è§£ç ”ç©¶ä¸»é¢˜ â†’ ç”Ÿæˆåˆå§‹æŸ¥è¯¢
    2. æ‰§è¡Œæœç´¢ â†’ æ”¶é›†æ¥æº
    3. è¯„ä¼°æ¥æº â†’ ç­›é€‰é«˜è´¨é‡æ¥æº
    4. æ·±åº¦é˜…è¯» â†’ æå–å…³é”®å‘ç°
    5. æ‰©å±•æŸ¥è¯¢ â†’ å¡«è¡¥çŸ¥è¯†ç©ºç™½
    6. ä¿¡æ¯ç»¼åˆ â†’ ç”ŸæˆæŠ¥å‘Š
    """

    def __init__(self, *args, max_concurrent: int = MAX_CONCURRENT_TOOLS, **kwargs):
        super().__init__(*args, **kwargs)
        self.research_state: ResearchState | None = None
        self._action_count = 0  # ç”¨äº 2-Action Rule
        self._max_concurrent = max_concurrent  # æœ€å¤§å¹¶å‘æ•°
        self._semaphore = asyncio.Semaphore(max_concurrent)  # å¹¶å‘æ§åˆ¶
        self._pending_urls: list[str] = []  # å¾…å¹¶å‘è¯»å–çš„ URL
        self._pending_queries: list[str] = []  # å¾…å¹¶å‘æœç´¢çš„æŸ¥è¯¢
        self._user_interventions: list[dict] = []  # ç”¨æˆ·å¹²é¢„æŒ‡ä»¤
        self._skip_domains: list[str] = []  # è·³è¿‡çš„åŸŸå

    # ==================== ç”¨æˆ·å¹²é¢„å¤„ç† ====================

    async def check_interventions(self) -> AsyncGenerator[SSEEvent, None]:
        """æ£€æŸ¥ç”¨æˆ·å¹²é¢„æŒ‡ä»¤

        åœ¨æ¯æ¬¡å·¥å…·è°ƒç”¨å‰æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·å¹²é¢„ï¼Œå¹¶è°ƒæ•´ç ”ç©¶è¡Œä¸ºã€‚
        """
        from ...api.v1.research import get_interventions_for_session, mark_intervention_processed

        session_id = self.context.session_id
        interventions = get_interventions_for_session(session_id)

        for intervention in interventions:
            intervention_id = intervention.get('id')
            intervention_type = intervention.get('type')
            content = intervention.get('content', '')

            logger.info(f"Processing intervention: {intervention_type} - {content[:50]}")

            # å‘é€å¹²é¢„å¤„ç†äº‹ä»¶
            yield SSEEvent(
                type=SSEEventType.STATUS,
                data={'content': f'\U0001f4a1 ç”¨æˆ·å¹²é¢„: {content[:50]}...\n'}
            )

            # æ ¹æ®å¹²é¢„ç±»å‹è°ƒæ•´è¡Œä¸º
            if intervention_type == 'add_focus':
                # è¿½åŠ å…³æ³¨æ–¹å‘ï¼šæ·»åŠ åˆ°çŸ¥è¯†ç©ºç™½ä¸­
                if self.research_state:
                    self.research_state.knowledge_gaps.append(content)
                    # æ·»åŠ æ–°çš„æœç´¢æŸ¥è¯¢
                    self._pending_queries.append(content)

            elif intervention_type == 'skip_source':
                # è·³è¿‡æŸç±»æ¥æºï¼šæ·»åŠ åˆ°è·³è¿‡åˆ—è¡¨
                self._skip_domains.append(content.lower())

            elif intervention_type == 'add_query':
                # è¿½åŠ æœç´¢è¯
                self._pending_queries.append(content)

            elif intervention_type == 'change_depth':
                # è°ƒæ•´ç ”ç©¶æ·±åº¦
                if self.research_state and 'æµ…' in content or 'shallow' in content.lower():
                    self.research_state.max_sources = max(3, self.research_state.max_sources - 3)
                elif self.research_state and 'æ·±' in content or 'deep' in content.lower():
                    self.research_state.max_sources = min(20, self.research_state.max_sources + 5)

            elif intervention_type == 'stop_reading':
                # åœæ­¢é˜…è¯»å½“å‰æ¥æºï¼šæ¸…ç©ºå¾…è¯» URL
                self._pending_urls.clear()

            elif intervention_type == 'custom':
                # è‡ªå®šä¹‰æŒ‡ä»¤ï¼šå­˜å‚¨åˆ°ä¸Šä¸‹æ–‡ä¾› LLM å‚è€ƒ
                self._user_interventions.append({
                    'type': 'custom',
                    'content': content,
                    'timestamp': intervention.get('timestamp')
                })

            # æ ‡è®°å¹²é¢„å·²å¤„ç†
            mark_intervention_processed(session_id, intervention_id, 'applied')

    def _should_skip_url(self, url: str) -> bool:
        """æ£€æŸ¥ URL æ˜¯å¦åº”è¯¥è¢«è·³è¿‡"""
        url_lower = url.lower()
        for domain in self._skip_domains:
            if domain in url_lower:
                return True
        return False

    def get_user_intervention_context(self) -> str:
        """è·å–ç”¨æˆ·å¹²é¢„ä¸Šä¸‹æ–‡ï¼ˆä¾› LLM å‚è€ƒï¼‰"""
        if not self._user_interventions:
            return ""

        context_parts = ["\n[User Interventions during research:"]
        for i, intervention in enumerate(self._user_interventions, 1):
            context_parts.append(f"{i}. {intervention['content']}")
        context_parts.append("]\n")
        return "\n".join(context_parts)

    # ==================== è¦†å†™å·¥å…·æ‰§è¡Œä»¥å‘é€ Timeline äº‹ä»¶ ====================

    async def _execute_tool(
        self,
        action: AgentAction
    ) -> AsyncGenerator[SSEEvent, None]:
        """è¦†å†™ BaseAgent._execute_tool ä»¥åœ¨å·¥å…·æ‰§è¡Œå‰æ£€æŸ¥å¹²é¢„ï¼Œå·¥å…·æ‰§è¡Œåå‘é€ Timeline äº‹ä»¶"""
        # å·¥å…·æ‰§è¡Œå‰æ£€æŸ¥ç”¨æˆ·å¹²é¢„
        async for event in self.check_interventions():
            yield event

        tool_name = action.tool_name
        tool_args = action.tool_args or {}

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡è¯¥ URL
        if tool_name == 'read_url' and self._should_skip_url(tool_args.get('url', '')):
            logger.info(f"Skipping URL due to user intervention: {tool_args.get('url', '')}")
            yield SSEEvent(
                type=SSEEventType.STATUS,
                data={'content': f'\u23e9 æ ¹æ®ç”¨æˆ·æŒ‡ä»¤è·³è¿‡: {tool_args.get("url", "")[:50]}...\n'}
            )
            return

        # è°ƒç”¨çˆ¶ç±»çš„å·¥å…·æ‰§è¡Œé€»è¾‘
        tool_result = None
        async for event in super()._execute_tool(action):
            yield event
            # æ•è·å·¥å…·æ‰§è¡Œç»“æœ
            if event.type == SSEEventType.TOOL_RESULT and event.data.get('status') == 'success':
                tool_result = event.data.get('result', '')

        # å·¥å…·æ‰§è¡ŒæˆåŠŸåï¼Œå‘é€ Timeline äº‹ä»¶
        if tool_result is not None:
            if tool_name == "web_search":
                # å°è¯•è§£æç»“æœè·å–ç»“æœæ•°
                try:
                    import json
                    result_data = json.loads(tool_result) if isinstance(tool_result, str) else tool_result
                    results_count = len(result_data.get('results', [])) if isinstance(result_data, dict) else 0
                except Exception:
                    results_count = 0
                query = tool_args.get('query', '')
                yield self._emit_timeline_search(query, results_count)

            elif tool_name == "read_url":
                url = tool_args.get('url', '')
                # å°è¯•ä»ç»“æœä¸­æå–æ ‡é¢˜
                try:
                    import json
                    result_data = json.loads(tool_result) if isinstance(tool_result, str) else tool_result
                    title = result_data.get('title', 'Unknown') if isinstance(result_data, dict) else 'Unknown'
                except Exception:
                    title = 'Unknown'
                yield self._emit_timeline_read(url, title)

            elif tool_name == "browser_screenshot":
                try:
                    import json
                    result_data = json.loads(tool_result) if isinstance(tool_result, str) else tool_result
                    path = result_data.get('path', '') if isinstance(result_data, dict) else ''
                    url = result_data.get('url', '') if isinstance(result_data, dict) else ''
                except Exception:
                    path = ''
                    url = ''
                yield self._emit_timeline_screenshot(path, url)

    # ==================== Timeline äº‹ä»¶å‘é€ ====================

    def _emit_timeline_search(self, query: str, results_count: int) -> SSEEvent:
        """å‘é€æœç´¢ Timeline äº‹ä»¶"""
        return SSEEvent(
            type=SSEEventType.TIMELINE_SEARCH,
            data={
                'query': query,
                'results_count': results_count,
                'title': f"æœç´¢: {query[:50]}",
                'description': f"æœç´¢ '{query}' è¿”å› {results_count} æ¡ç»“æœ"
            }
        )

    def _emit_timeline_read(self, url: str, title: str) -> SSEEvent:
        """å‘é€é˜…è¯» Timeline äº‹ä»¶"""
        return SSEEvent(
            type=SSEEventType.TIMELINE_READ,
            data={
                'url': url,
                'title': f"é˜…è¯»: {title[:50]}",
                'description': f"é˜…è¯»å¹¶æå–å†…å®¹: {url}"
            }
        )

    def _emit_timeline_finding(self, finding: str, source_url: str | None = None) -> SSEEvent:
        """å‘é€å‘ç° Timeline äº‹ä»¶"""
        return SSEEvent(
            type=SSEEventType.TIMELINE_FINDING,
            data={
                'content': finding,
                'source_url': source_url,
                'title': f"å‘ç°: {finding[:50]}",
                'description': finding
            }
        )

    def _emit_timeline_milestone(self, milestone: str, description: str = "") -> SSEEvent:
        """å‘é€é‡Œç¨‹ç¢‘ Timeline äº‹ä»¶"""
        return SSEEvent(
            type=SSEEventType.TIMELINE_MILESTONE,
            data={
                'title': milestone,
                'description': description or milestone
            }
        )

    def _emit_timeline_screenshot(self, path: str, url: str | None = None, title: str = "") -> SSEEvent:
        """å‘é€æˆªå›¾ Timeline äº‹ä»¶"""
        return SSEEvent(
            type=SSEEventType.TIMELINE_SCREENSHOT,
            data={
                'path': path,
                'url': url,
                'title': f"æˆªå›¾: {title[:50]}" if title else "æˆªå›¾",
                'description': f"é¡µé¢æˆªå›¾: {url or path}"
            }
        )

    # ==================== Research Progress äº‹ä»¶å‘é€ ====================

    def _emit_phase_change(self, phase: str, phase_progress: int = 0) -> SSEEvent:
        """å‘é€é˜¶æ®µåˆ‡æ¢äº‹ä»¶"""
        phase_names = {
            "planning": "è§„åˆ’",
            "searching": "æœç´¢",
            "reading": "é˜…è¯»",
            "analyzing": "åˆ†æ",
            "writing": "æ’°å†™",
        }
        return SSEEvent(
            type=SSEEventType.RESEARCH_PHASE_CHANGE,
            data={
                'phase': phase,
                'phase_name': phase_names.get(phase, phase),
                'phase_progress': phase_progress,
            }
        )

    def _emit_query_start(self, query_id: str, query_text: str) -> SSEEvent:
        """å‘é€æœç´¢å¼€å§‹äº‹ä»¶"""
        return SSEEvent(
            type=SSEEventType.RESEARCH_QUERY_START,
            data={
                'query_id': query_id,
                'text': query_text,
                'status': 'running',
            }
        )

    def _emit_query_result(self, query_id: str, query_text: str, result_count: int) -> SSEEvent:
        """å‘é€æœç´¢ç»“æœäº‹ä»¶"""
        return SSEEvent(
            type=SSEEventType.RESEARCH_QUERY_RESULT,
            data={
                'query_id': query_id,
                'text': query_text,
                'status': 'done',
                'result_count': result_count,
            }
        )

    def _emit_source_start(self, source_id: str, url: str, domain: str, title: str = "") -> SSEEvent:
        """å‘é€æ¥æºé˜…è¯»å¼€å§‹äº‹ä»¶"""
        return SSEEvent(
            type=SSEEventType.RESEARCH_SOURCE_START,
            data={
                'source_id': source_id,
                'url': url,
                'domain': domain,
                'title': title,
                'status': 'reading',
            }
        )

    def _emit_source_done(
        self,
        source_id: str,
        url: str,
        domain: str,
        title: str,
        credibility: int,
        source_type: str = "unknown",
        extracted_facts: list[str] | None = None
    ) -> SSEEvent:
        """å‘é€æ¥æºé˜…è¯»å®Œæˆäº‹ä»¶"""
        return SSEEvent(
            type=SSEEventType.RESEARCH_SOURCE_DONE,
            data={
                'source_id': source_id,
                'url': url,
                'domain': domain,
                'title': title,
                'credibility': credibility,
                'type': source_type,
                'status': 'done',
                'extracted_facts': extracted_facts or [],
            }
        )

    def _emit_progress_update(
        self,
        phase: str,
        phase_progress: int,
        overall_progress: int,
        current_action: str = "",
        estimated_time: int | None = None
    ) -> SSEEvent:
        """å‘é€è¿›åº¦æ›´æ–°äº‹ä»¶"""
        return SSEEvent(
            type=SSEEventType.RESEARCH_PROGRESS_UPDATE,
            data={
                'phase': phase,
                'phase_progress': phase_progress,
                'overall_progress': overall_progress,
                'current_action': current_action,
                'estimated_time_remaining': estimated_time,
            }
        )

    async def _think(self) -> AsyncGenerator[SSEEvent, None]:
        """æ€è€ƒè¿‡ç¨‹ - DeepResearch ç‰ˆæœ¬

        æ ¹æ®å½“å‰ç ”ç©¶é˜¶æ®µè¿›è¡Œé’ˆå¯¹æ€§æ€è€ƒ
        """
        logger.debug("DeepResearchAgent thinking...")

        phase = self.research_state.phase if self.research_state else "init"

        # é˜¶æ®µæ€§æ€è€ƒæç¤º
        thinking_prompts = {
            "init": "ğŸ¯ Analyzing research topic and planning initial search strategy...",
            "searching": "ğŸ” Evaluating search results and identifying knowledge gaps...",
            "reading": "ğŸ“– Extracting key findings and assessing source credibility...",
            "synthesizing": "ğŸ”— Synthesizing information and resolving conflicts...",
            "reporting": "ğŸ“ Generating structured research report..."
        }

        yield SSEEvent(
            type=SSEEventType.THINKING,
            data={'content': thinking_prompts.get(phase, "Thinking...") + "\n"}
        )

        # æ„é€ æ€è€ƒç³»ç»Ÿæç¤º
        system_prompt = self._get_thinking_prompt(phase)

        # ä½¿ç”¨ LLM è¿›è¡Œæ€è€ƒ
        thinking_content = ""
        async for chunk in self.llm.stream(
            messages=self.context.messages,
            system=system_prompt
        ):
            thinking_content += chunk
            yield SSEEvent(
                type=SSEEventType.THINKING,
                data={'content': chunk}
            )

        # ä¿å­˜æ€è€ƒå†…å®¹
        self.context.append_thinking(thinking_content)

        # è§£ææ€è€ƒç»“æœï¼Œæ›´æ–°ç ”ç©¶çŠ¶æ€
        await self._update_state_from_thinking(thinking_content)

        logger.debug(f"Thinking complete, phase: {phase}")

    def _get_thinking_prompt(self, phase: str) -> str:
        """è·å–é˜¶æ®µæ€§æ€è€ƒæç¤º"""

        base_prompt = """You are a deep research assistant conducting systematic research.

Current Research State:
"""
        if self.research_state:
            base_prompt += f"""- Topic: {self.research_state.topic}
- Phase: {phase}
- Sources collected: {len(self.research_state.sources_collected)}
- Queries executed: {len(self.research_state.queries_executed)}
- Knowledge gaps: {self.research_state.knowledge_gaps}
"""

        phase_prompts = {
            "init": """
Analyze the research topic and:
1. Break down the topic into key aspects to investigate
2. Generate 3-5 specific search queries
3. Identify potential authoritative sources

Output your thinking as structured analysis.""",

            "searching": """
Evaluate the current search results:
1. Assess source credibility (authoritative, reliable, moderate, questionable)
2. Identify knowledge gaps - what aspects are not yet covered?
3. Generate follow-up queries to fill gaps

Focus on finding diverse, high-quality sources.""",

            "reading": """
For each source being read:
1. Extract key findings and claims
2. Note any data, statistics, or quotes
3. Identify conflicting information
4. Rate information completeness

Be thorough but concise.""",

            "synthesizing": """
Synthesize all collected information:
1. Identify consensus across sources
2. Note conflicting viewpoints and their sources
3. Assess overall confidence in findings
4. Structure the main conclusions

Prepare for report generation.""",

            "reporting": """
Plan the research report structure:
1. Executive summary
2. Key findings by aspect
3. Conflicting viewpoints
4. Limitations and gaps
5. References

Ensure all claims have citations."""
        }

        return base_prompt + phase_prompts.get(phase, "Analyze the current situation.")

    async def _update_state_from_thinking(self, thinking: str) -> None:
        """ä»æ€è€ƒå†…å®¹æ›´æ–°ç ”ç©¶çŠ¶æ€"""
        if not self.research_state:
            return

        # ç®€å•çš„çŠ¶æ€è½¬æ¢é€»è¾‘ï¼ˆå¯ä»¥ç”¨ LLM è§£ææ›´å¤æ‚çš„å†…å®¹ï¼‰
        if self.research_state.phase == "init":
            # æå–æŸ¥è¯¢å»ºè®®
            queries = self._extract_queries_from_thinking(thinking)
            if queries:
                self.research_state.knowledge_gaps = queries

        elif self.research_state.phase == "searching":
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›å…¥ä¸‹ä¸€é˜¶æ®µ
            if len(self.research_state.sources_collected) >= self.research_state.min_credible_sources:
                # æœ‰è¶³å¤Ÿæ¥æºï¼Œå¯ä»¥å¼€å§‹æ·±åº¦é˜…è¯»
                pass

    def _extract_queries_from_thinking(self, thinking: str) -> list[str]:
        """ä»æ€è€ƒå†…å®¹æå–æŸ¥è¯¢å»ºè®®"""
        queries = []
        # ç®€å•æ­£åˆ™åŒ¹é…æ•°å­—åˆ—è¡¨æ ¼å¼
        pattern = r'(?:^|\n)\s*\d+[\.\)]\s*(.+?)(?=\n\s*\d+[\.\)]|\n\n|$)'
        matches = re.findall(pattern, thinking, re.MULTILINE)
        for match in matches[:5]:  # æœ€å¤š5ä¸ª
            query = match.strip().strip('"\'')
            if len(query) > 10 and len(query) < 200:
                queries.append(query)
        return queries

    async def _decide(self) -> AgentAction:
        """å†³ç­– - DeepResearch ç‰ˆæœ¬

        æ ¹æ®ç ”ç©¶é˜¶æ®µå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨
        """
        logger.debug(f"DeepResearchAgent deciding, phase: {self.research_state.phase if self.research_state else 'init'}")

        # åˆå§‹åŒ–ç ”ç©¶çŠ¶æ€
        if not self.research_state:
            topic = self._extract_topic_from_context()
            self.research_state = ResearchState(topic=topic)
            logger.info(f"Research initialized for topic: {topic}")

        # è·å–å¯ç”¨å·¥å…·
        tool_definitions = self.tools.to_llm_format()

        if not tool_definitions:
            logger.warning("No tools available, generating answer directly")
            return await self._generate_final_report()

        # æ ¹æ®é˜¶æ®µå†³å®šç³»ç»Ÿæç¤º
        system_prompt = self._get_decision_prompt()

        # è°ƒç”¨ LLM è¿›è¡Œå†³ç­–
        response = await self.llm.complete(
            messages=self.context.messages,
            system=system_prompt,
            tools=tool_definitions
        )

        # å¤„ç†å·¥å…·è°ƒç”¨
        if response.tool_calls:
            tool_call = response.tool_calls[0]

            # æ›´æ–° action count (2-Action Rule)
            self._action_count += 1

            # æ¯2æ¬¡é‡å¤§æ“ä½œåï¼Œè§¦å‘å†™å…¥ findings
            if self._action_count >= 2 and tool_call["name"] in ["web_search", "read_url", "browser_open"]:
                self._action_count = 0
                # è®°å½•åˆ° findings.md
                await self._record_findings()

            # æ›´æ–°é˜¶æ®µ
            self._update_phase_from_tool(tool_call["name"])

            return AgentAction(
                type=ActionType.TOOL_CALL,
                tool_name=tool_call["name"],
                tool_args=tool_call["input"],
                tool_call_id=tool_call["id"]
            )

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç”ŸæˆæŠ¥å‘Š
        if self._should_generate_report():
            return await self._generate_final_report()

        # é»˜è®¤ç»§ç»­ç ”ç©¶
        answer = response.content.strip()
        return AgentAction(
            type=ActionType.ANSWER,
            answer=answer
        )

    def _get_decision_prompt(self) -> str:
        """è·å–å†³ç­–æç¤º"""
        phase = self.research_state.phase if self.research_state else "init"

        base = """You are conducting deep research. Based on the conversation and your analysis:

Available tools:
- web_search: Search for information (use for broad queries)
- read_url: Read full content from a URL (use for deep reading)
- browser_open: Open page with interactive elements (use for dynamic pages)
- browser_screenshot: Capture page screenshot (use for timeline)

Current research state:
"""
        if self.research_state:
            base += f"""- Topic: {self.research_state.topic}
- Phase: {phase}
- Sources: {len(self.research_state.sources_collected)}/{self.research_state.max_sources}
- Queries done: {self.research_state.queries_executed}
"""

        phase_instructions = {
            "init": """
You are starting research. Your first action should be:
1. Use web_search with a well-crafted query about the topic
2. Focus on finding authoritative sources first

Do NOT generate a final answer yet - gather information first.""",

            "searching": """
Continue gathering sources:
1. If you have < 3 credible sources, continue searching with refined queries
2. If you found good sources, use read_url to get full content
3. Consider different perspectives on the topic

Prioritize quality over quantity.""",

            "reading": """
You are in deep reading phase:
1. Use read_url to get full content from promising sources
2. Use browser_screenshot to capture important pages for timeline
3. Once you have enough content, move to synthesis

Extract specific facts, quotes, and data.""",

            "synthesizing": """
You have gathered enough information. Now:
1. If any critical gaps remain, do one more targeted search
2. Otherwise, generate a comprehensive research report

Your report should include:
- Executive Summary
- Key Findings (with citations)
- Conflicting Viewpoints
- Limitations
- References""",

            "reporting": """
Generate the final research report now.
Include all citations and organize findings clearly.
Do NOT call any more tools - provide the complete answer."""
        }

        return base + phase_instructions.get(phase, "Continue research based on current needs.")

    def _update_phase_from_tool(self, tool_name: str) -> None:
        """æ ¹æ®å·¥å…·è°ƒç”¨æ›´æ–°é˜¶æ®µ"""
        if not self.research_state:
            return

        if tool_name == "web_search":
            if self.research_state.phase == "init":
                self.research_state.phase = "searching"

        elif tool_name in ["read_url", "browser_open"]:
            if self.research_state.phase in ["init", "searching"]:
                self.research_state.phase = "reading"

        elif tool_name == "browser_screenshot":
            # æˆªå›¾ä¸æ”¹å˜é˜¶æ®µ
            pass

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿›å…¥ç»¼åˆé˜¶æ®µ
        if len(self.research_state.sources_collected) >= self.research_state.min_credible_sources:
            if self.research_state.phase == "reading":
                self.research_state.phase = "synthesizing"

    def _should_generate_report(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç”ŸæˆæŠ¥å‘Š"""
        if not self.research_state:
            return False

        # å·²ç»åœ¨æŠ¥å‘Šé˜¶æ®µ
        if self.research_state.phase == "reporting":
            return True

        # è¾¾åˆ°æœ€å¤§æ¥æºæ•°
        if len(self.research_state.sources_collected) >= self.research_state.max_sources:
            self.research_state.phase = "reporting"
            return True

        # è¿­ä»£æ¬¡æ•°è¿‡å¤š
        if self.research_state.iteration > 15:
            self.research_state.phase = "reporting"
            return True

        return False

    async def _generate_final_report(self) -> AgentAction:
        """ç”Ÿæˆæœ€ç»ˆç ”ç©¶æŠ¥å‘Š"""
        logger.info("Generating final research report...")

        # æ„é€ æŠ¥å‘Šç”Ÿæˆæç¤º
        system_prompt = """Generate a comprehensive research report based on all gathered information.

Structure:
1. **Executive Summary** (2-3 sentences)
2. **Key Findings** (numbered list with source citations [1], [2], etc.)
3. **Analysis** (synthesize the findings)
4. **Conflicting Viewpoints** (if any)
5. **Limitations & Gaps** (what couldn't be determined)
6. **References** (numbered list of all sources)

Use markdown formatting. Be thorough but concise.
Every factual claim MUST have a citation."""

        # æ·»åŠ æ¥æºä¿¡æ¯åˆ°ä¸Šä¸‹æ–‡
        sources_context = self._format_sources_for_report()

        messages = self.context.messages.copy()
        messages.append({
            "role": "user",
            "content": f"Based on the research, generate the final report.\n\nCollected Sources:\n{sources_context}"
        })

        response = await self.llm.complete(
            messages=messages,
            system=system_prompt
        )

        report = response.content.strip()

        # æ·»åŠ å¼•ç”¨éƒ¨åˆ†
        citations: list[Citation] = []
        if self.research_state and self.research_state.sources_collected:
            report += "\n\n---\n\n## References\n\n"
            for i, source in enumerate(self.research_state.sources_collected, 1):
                report += source.to_citation(i) + "\n"
                # æ„å»º Citation å¯¹è±¡
                citations.append(self._build_citation(i, source))

        # è®°å½•åˆ° findings.md
        await self.memory.write_findings(
            "Research Report Generated",
            report[:2000]  # æ‘˜è¦
        )

        self.research_state.phase = "reporting"

        return AgentAction(
            type=ActionType.ANSWER,
            answer=report,
            data={
                "report_type": "research",
                "citations": [c.to_dict() for c in citations],
            }
        )

    def _build_citation(self, index: int, source: ResearchSource) -> Citation:
        """ä» ResearchSource æ„å»º Citation"""
        # å°† SourceCredibility è½¬æ¢ä¸ºæ•°å€¼
        credibility_map = {
            SourceCredibility.AUTHORITATIVE: 95,
            SourceCredibility.RELIABLE: 75,
            SourceCredibility.MODERATE: 50,
            SourceCredibility.QUESTIONABLE: 30,
            SourceCredibility.UNRELIABLE: 10,
        }
        credibility_score = credibility_map.get(source.credibility, 50)

        # æå–åŸŸå
        from urllib.parse import urlparse
        domain = urlparse(source.url).netloc or source.url[:50]

        citation_source = CitationSource(
            url=source.url,
            title=source.title,
            domain=domain,
            publish_date=source.timestamp.strftime("%Y-%m-%d") if source.timestamp else None,
            credibility=credibility_score,
            source_type=self._infer_source_type(source.url, source.credibility),
        )

        return Citation(
            id=index,
            source=citation_source,
            excerpt=source.snippet[:300] if source.snippet else "",
            excerpt_context=source.content[:500] if source.content else "",
            claim_text="",  # å¯åç»­ä»æŠ¥å‘Šä¸­æå–
        )

    def _infer_source_type(self, url: str, credibility: SourceCredibility) -> str:
        """æ¨æ–­æ¥æºç±»å‹"""
        url_lower = url.lower()

        if "arxiv.org" in url_lower or ".edu" in url_lower or "ieee.org" in url_lower:
            return "academic"
        if "gartner.com" in url_lower or "mckinsey.com" in url_lower or "report" in url_lower:
            return "report"
        if any(site in url_lower for site in ["nytimes", "bbc", "reuters", "techcrunch", "wired"]):
            return "news"
        if any(site in url_lower for site in ["medium.com", "dev.to", "blog"]):
            return "blog"
        if ".gov" in url_lower or "docs." in url_lower or "developer." in url_lower:
            return "official"

        return "unknown"

    def _format_sources_for_report(self) -> str:
        """æ ¼å¼åŒ–æ¥æºä¿¡æ¯ç”¨äºæŠ¥å‘Šç”Ÿæˆ"""
        if not self.research_state or not self.research_state.sources_collected:
            return "No sources collected yet."

        formatted = ""
        for i, source in enumerate(self.research_state.sources_collected, 1):
            formatted += f"\n[{i}] {source.title}\n"
            formatted += f"    URL: {source.url}\n"
            formatted += f"    Credibility: {source.credibility.value}\n"
            if source.key_findings:
                formatted += "    Key Findings:\n"
                for finding in source.key_findings:
                    formatted += f"      - {finding}\n"
            if source.snippet:
                formatted += f"    Snippet: {source.snippet[:300]}...\n"

        return formatted

    async def _record_findings(self) -> None:
        """è®°å½•å‘ç°åˆ° findings.md (2-Action Rule)"""
        if not self.research_state:
            return

        summary = f"Research Progress - Phase: {self.research_state.phase}\n"
        summary += f"Sources: {len(self.research_state.sources_collected)}\n"
        summary += f"Queries: {self.research_state.queries_executed[-3:]}\n"  # æœ€è¿‘3ä¸ªæŸ¥è¯¢

        try:
            await self.memory.write_findings(
                f"Research Update ({self.research_state.topic[:50]})",
                summary
            )
        except Exception as e:
            logger.warning(f"Failed to record findings: {e}")

    def _extract_topic_from_context(self) -> str:
        """ä»ä¸Šä¸‹æ–‡æå–ç ”ç©¶ä¸»é¢˜"""
        if not self.context.messages:
            return "Unknown topic"

        # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        for msg in reversed(self.context.messages):
            if msg.get("role") == "user":
                content = msg.get("content", "")
                # æ¸…ç†å†…å®¹ï¼Œæå–ä¸»è¦ä¸»é¢˜
                topic = content.strip()[:200]
                return topic

        return "Unknown topic"

    # ==================== æ¥æºå¯ä¿¡åº¦è¯„ä¼° ====================

    def assess_source_credibility(self, url: str, title: str) -> SourceCredibility:
        """è¯„ä¼°æ¥æºå¯ä¿¡åº¦

        åŸºäº URL åŸŸåå’Œæ ‡é¢˜è¿›è¡Œåˆæ­¥è¯„ä¼°
        """
        url_lower = url.lower()

        # æƒå¨æ¥æºæ¨¡å¼
        authoritative_patterns = [
            ".gov", ".edu", "arxiv.org", "nature.com", "science.org",
            "ieee.org", "acm.org", "springer.com", "wiley.com",
            "nih.gov", "who.int", "un.org"
        ]

        # å¯é æ¥æºæ¨¡å¼
        reliable_patterns = [
            "github.com", "stackoverflow.com", "medium.com",
            "nytimes.com", "bbc.com", "reuters.com", "theguardian.com",
            "techcrunch.com", "wired.com", "arstechnica.com",
            "docs.python.org", "docs.microsoft.com", "developer.mozilla.org"
        ]

        # æ£€æŸ¥æƒå¨æ¥æº
        for pattern in authoritative_patterns:
            if pattern in url_lower:
                return SourceCredibility.AUTHORITATIVE

        # æ£€æŸ¥å¯é æ¥æº
        for pattern in reliable_patterns:
            if pattern in url_lower:
                return SourceCredibility.RELIABLE

        # é»˜è®¤ä¸ºä¸€èˆ¬å¯ä¿¡åº¦
        return SourceCredibility.MODERATE

    def add_source(self, url: str, title: str, snippet: str) -> ResearchSource:
        """æ·»åŠ ç ”ç©¶æ¥æº"""
        credibility = self.assess_source_credibility(url, title)

        source = ResearchSource(
            url=url,
            title=title,
            snippet=snippet,
            credibility=credibility
        )

        if self.research_state:
            self.research_state.sources_collected.append(source)
            logger.info(f"Added source: {title} (credibility: {credibility.value})")

        return source

    # ==================== å¹¶å‘æ‰§è¡Œæ”¯æŒ ====================

    async def _execute_tool_with_semaphore(
        self,
        tool_name: str,
        tool_input: dict[str, Any]
    ) -> tuple[str, dict[str, Any]]:
        """å¸¦ä¿¡å·é‡æ§åˆ¶çš„å·¥å…·æ‰§è¡Œ

        Args:
            tool_name: å·¥å…·åç§°
            tool_input: å·¥å…·è¾“å…¥å‚æ•°

        Returns:
            Tuple[str, Dict]: (å·¥å…·å, æ‰§è¡Œç»“æœ)
        """
        async with self._semaphore:
            try:
                tool = self.tools.get(tool_name)
                if not tool:
                    return (tool_name, {"success": False, "error": f"Tool {tool_name} not found"})

                result = await tool.execute(**tool_input)
                return (tool_name, result)
            except Exception as e:
                logger.error(f"Tool {tool_name} execution failed: {e}")
                return (tool_name, {"success": False, "error": str(e)})

    async def execute_tools_concurrently(
        self,
        tool_calls: list[tuple[str, dict[str, Any]]]
    ) -> AsyncGenerator[SSEEvent, None]:
        """å¹¶å‘æ‰§è¡Œå¤šä¸ªå·¥å…·

        Args:
            tool_calls: List of (tool_name, tool_input) tuples

        Yields:
            SSEEvent: æ‰§è¡Œè¿›åº¦å’Œç»“æœäº‹ä»¶
        """
        if not tool_calls:
            return

        # é™åˆ¶å¹¶å‘æ•°
        batch_size = min(len(tool_calls), self._max_concurrent)

        yield SSEEvent(
            type=SSEEventType.STATUS,
            data={
                'content': f'ğŸš€ Executing {len(tool_calls)} tools concurrently (max {batch_size} parallel)...\n'
            }
        )

        # åˆ›å»ºä»»åŠ¡
        tasks = [
            self._execute_tool_with_semaphore(name, inputs)
            for name, inputs in tool_calls
        ]

        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†ç»“æœ
        successful = 0
        failed = 0

        for i, result in enumerate(results):
            tool_name, tool_input = tool_calls[i]

            if isinstance(result, Exception):
                failed += 1
                yield SSEEvent(
                    type=SSEEventType.TOOL_ERROR,
                    data={
                        'tool': tool_name,
                        'error': str(result)
                    }
                )
            else:
                name, output = result
                if output.get("success", False):
                    successful += 1
                    # å¤„ç†æœç´¢ç»“æœ - æ”¶é›† URL
                    if name == "web_search" and output.get("results"):
                        for item in output.get("results", []):
                            url = item.get("link") or item.get("url")
                            if url:
                                self._pending_urls.append(url)

                    # å¤„ç† read_url ç»“æœ - æ·»åŠ æ¥æº
                    if name == "read_url" and output.get("content"):
                        self.add_source(
                            url=output.get("url", ""),
                            title=output.get("title", "Unknown"),
                            snippet=output.get("content", "")[:500]
                        )
                else:
                    failed += 1

                yield SSEEvent(
                    type=SSEEventType.TOOL_RESULT,
                    data={
                        'tool': name,
                        'result': output,
                        'success': output.get("success", False)
                    }
                )

                # å‘é€ Timeline äº‹ä»¶ (æ—¶å…‰é•¿å»Š)
                if output.get("success", False):
                    if name == "web_search":
                        query = tool_input.get("query", "")
                        results_count = len(output.get("results", []))
                        yield self._emit_timeline_search(query, results_count)
                    elif name == "read_url":
                        url = output.get("url", tool_input.get("url", ""))
                        title = output.get("title", "Unknown")
                        yield self._emit_timeline_read(url, title)
                    elif name == "browser_screenshot":
                        path = output.get("path", "")
                        url = output.get("url", "")
                        yield self._emit_timeline_screenshot(path, url)

        yield SSEEvent(
            type=SSEEventType.STATUS,
            data={
                'content': f'âœ… Concurrent execution complete: {successful} succeeded, {failed} failed\n'
            }
        )

        # æ›´æ–° action count
        self._action_count += len(tool_calls)

        # 2-Action Rule: æ¯ 2 æ¬¡é‡å¤§æ“ä½œåå†™å…¥ findings
        if self._action_count >= 2:
            self._action_count = 0
            await self._record_findings()

    async def batch_search(self, queries: list[str]) -> AsyncGenerator[SSEEvent, None]:
        """æ‰¹é‡å¹¶å‘æœç´¢

        Args:
            queries: æœç´¢æŸ¥è¯¢åˆ—è¡¨

        Yields:
            SSEEvent: æ‰§è¡Œäº‹ä»¶
        """
        tool_calls = [
            ("web_search", {"query": q, "max_results": 5})
            for q in queries[:self._max_concurrent]  # é™åˆ¶æ•°é‡
        ]

        if self.research_state:
            self.research_state.queries_executed.extend(queries[:self._max_concurrent])

        async for event in self.execute_tools_concurrently(tool_calls):
            yield event

    async def batch_read_urls(self, urls: list[str]) -> AsyncGenerator[SSEEvent, None]:
        """æ‰¹é‡å¹¶å‘è¯»å– URL

        Args:
            urls: URL åˆ—è¡¨

        Yields:
            SSEEvent: æ‰§è¡Œäº‹ä»¶
        """
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_urls = list(dict.fromkeys(urls))[:self._max_concurrent]

        tool_calls = [
            ("read_url", {"url": url, "use_jina": True, "max_length": 8000})
            for url in unique_urls
        ]

        async for event in self.execute_tools_concurrently(tool_calls):
            yield event

    def get_pending_urls(self) -> list[str]:
        """è·å–å¾…è¯»å–çš„ URL å¹¶æ¸…ç©º"""
        urls = self._pending_urls.copy()
        self._pending_urls.clear()
        return urls

    def queue_urls_for_reading(self, urls: list[str]) -> None:
        """å°† URL åŠ å…¥å¾…è¯»å–é˜Ÿåˆ—"""
        self._pending_urls.extend(urls)

    # ==================== æµå¼è¿”å› (Streaming) ====================

    async def execute_tools_streaming(
        self,
        tool_calls: list[tuple[str, dict[str, Any]]]
    ) -> AsyncGenerator[SSEEvent, None]:
        """æµå¼æ‰§è¡Œå·¥å…· - ä½¿ç”¨ asyncio.as_completed å®æ—¶è¿”å›

        ä¸ execute_tools_concurrently çš„åŒºåˆ«:
        - as_completed: å“ªä¸ªå…ˆå®Œæˆå°±å…ˆè¿”å›ï¼Œç”¨æˆ·æ›´å¿«çœ‹åˆ°ç»“æœ
        - gather: ç­‰æ‰€æœ‰å®Œæˆåä¸€èµ·è¿”å›

        Args:
            tool_calls: List of (tool_name, tool_input) tuples

        Yields:
            SSEEvent: å®æ—¶æ‰§è¡Œç»“æœ
        """
        if not tool_calls:
            return

        yield SSEEvent(
            type=SSEEventType.STATUS,
            data={
                'content': f'ğŸš€ Streaming {len(tool_calls)} tools (results as they complete)...\n'
            }
        )

        # åˆ›å»ºå¸¦ç´¢å¼•çš„ä»»åŠ¡
        async def execute_with_index(idx: int, name: str, inputs: dict) -> tuple[int, str, dict]:
            result = await self._execute_tool_with_semaphore(name, inputs)
            return (idx, result[0], result[1])

        tasks = [
            asyncio.create_task(execute_with_index(i, name, inputs))
            for i, (name, inputs) in enumerate(tool_calls)
        ]

        completed = 0
        successful = 0
        failed = 0

        # ä½¿ç”¨ as_completed å®ç°æµå¼è¿”å›
        for coro in asyncio.as_completed(tasks):
            try:
                idx, tool_name, result = await coro
                completed += 1

                if isinstance(result, dict) and result.get("success", False):
                    successful += 1

                    # å¤„ç†æœç´¢ç»“æœ
                    if tool_name == "web_search" and result.get("results"):
                        for item in result.get("results", []):
                            url = item.get("link") or item.get("url")
                            if url:
                                self._pending_urls.append(url)

                    # å¤„ç† read_url ç»“æœ
                    if tool_name == "read_url" and result.get("content"):
                        self.add_source(
                            url=result.get("url", ""),
                            title=result.get("title", "Unknown"),
                            snippet=result.get("content", "")[:500]
                        )
                else:
                    failed += 1

                # å®æ—¶æ¨é€ç»“æœ
                yield SSEEvent(
                    type=SSEEventType.TOOL_RESULT,
                    data={
                        'tool': tool_name,
                        'result': result,
                        'success': result.get("success", False) if isinstance(result, dict) else False,
                        'progress': f"{completed}/{len(tool_calls)}"
                    }
                )

                # å‘é€ Timeline äº‹ä»¶ (æ—¶å…‰é•¿å»Š)
                if isinstance(result, dict) and result.get("success", False):
                    if tool_name == "web_search":
                        query = tool_calls[idx][1].get("query", "")
                        results_count = len(result.get("results", []))
                        yield self._emit_timeline_search(query, results_count)
                    elif tool_name == "read_url":
                        url = result.get("url", tool_calls[idx][1].get("url", ""))
                        title = result.get("title", "Unknown")
                        yield self._emit_timeline_read(url, title)
                    elif tool_name == "browser_screenshot":
                        path = result.get("path", "")
                        url = result.get("url", "")
                        yield self._emit_timeline_screenshot(path, url)

            except Exception as e:
                completed += 1
                failed += 1
                logger.error(f"Streaming task error: {e}")
                yield SSEEvent(
                    type=SSEEventType.TOOL_ERROR,
                    data={'error': str(e), 'progress': f"{completed}/{len(tool_calls)}"}
                )

        yield SSEEvent(
            type=SSEEventType.STATUS,
            data={
                'content': f'âœ… Streaming complete: {successful} succeeded, {failed} failed\n'
            }
        )

        # æ›´æ–° action count
        self._action_count += len(tool_calls)
        if self._action_count >= 2:
            self._action_count = 0
            await self._record_findings()

    async def batch_search_streaming(
        self,
        queries: list[str]
    ) -> AsyncGenerator[SSEEvent, None]:
        """æµå¼æ‰¹é‡æœç´¢ - ç»“æœå®æ—¶è¿”å›

        ç”¨æˆ·ä½“éªŒä¼˜åŒ–:
        - é¦–ä¸ªç»“æœå»¶è¿Ÿ: ~2s (ä¹‹å‰éœ€è¦ç­‰æ‰€æœ‰å®Œæˆ: ~10s)
        """
        tool_calls = [
            ("web_search", {"query": q, "max_results": 5})
            for q in queries[:self._max_concurrent]
        ]

        if self.research_state:
            self.research_state.queries_executed.extend(queries[:self._max_concurrent])

        async for event in self.execute_tools_streaming(tool_calls):
            yield event

    async def batch_read_urls_streaming(
        self,
        urls: list[str],
        query: str | None = None
    ) -> AsyncGenerator[SSEEvent, None]:
        """æµå¼æ‰¹é‡è¯»å– URL

        Args:
            urls: URL åˆ—è¡¨
            query: ç ”ç©¶æŸ¥è¯¢ (ç”¨äº extract_relevant)
        """
        unique_urls = list(dict.fromkeys(urls))[:self._max_concurrent]

        tool_calls = [
            ("read_url", {
                "url": url,
                "use_jina": True,
                "extract_relevant": bool(query),
                "query": query or "",
                "max_length": 8000
            })
            for url in unique_urls
        ]

        async for event in self.execute_tools_streaming(tool_calls):
            yield event


# ==================== å·¥å‚å‡½æ•° ====================

async def create_deep_research_agent(
    context,
    llm,
    tools,
    memory,
    db,
    max_iterations: int = 30,
    max_sources: int = 10,
    max_concurrent: int = MAX_CONCURRENT_TOOLS
) -> DeepResearchAgent:
    """åˆ›å»º DeepResearchAgent å®ä¾‹

    Args:
        context: AgentContext
        llm: BaseLLM
        tools: ToolRegistry
        memory: WorkingMemory
        db: AsyncSession
        max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆæ·±åº¦ç ”ç©¶éœ€è¦æ›´å¤šè¿­ä»£ï¼‰
        max_sources: æœ€å¤§æ¥æºæ•°
        max_concurrent: æœ€å¤§å¹¶å‘å·¥å…·æ‰§è¡Œæ•° (é»˜è®¤ 10)

    Returns:
        DeepResearchAgent: Agent å®ä¾‹
    """
    agent = DeepResearchAgent(
        context=context,
        llm=llm,
        tools=tools,
        memory=memory,
        db=db,
        max_iterations=max_iterations,
        max_concurrent=max_concurrent
    )

    # åˆå§‹åŒ–ç ”ç©¶çŠ¶æ€å‚æ•°
    if agent.research_state:
        agent.research_state.max_sources = max_sources

    logger.info(
        f"DeepResearchAgent created with max_iterations={max_iterations}, "
        f"max_sources={max_sources}, max_concurrent={max_concurrent}"
    )
    return agent
