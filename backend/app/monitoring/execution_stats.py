"""
Agent æ‰§è¡Œç»Ÿè®¡å’Œç›‘æŽ§æ¨¡å—

è´Ÿè´£è¿½è¸ªå’Œåˆ†æž Agent çš„æ‰§è¡Œæ€§èƒ½ï¼ŒåŒ…æ‹¬ï¼š
- æ‰§è¡Œè·¯å¾„åˆ†å¸ƒï¼ˆSkill/MCP/LLMï¼‰
- æˆåŠŸçŽ‡ç»Ÿè®¡ï¼ˆæŒ‰è·¯å¾„åˆ†åˆ«è®¡ç®—ï¼‰
- æ‰§è¡Œå»¶è¿Ÿè¿½è¸ªï¼ˆå¹³å‡/æœ€å°/æœ€å¤§ï¼‰
- é”™è¯¯åˆ†æžå’Œåˆ†ç±»
- å®žæ—¶æ€§èƒ½æŠ¥å‘Šç”Ÿæˆ
"""

import json
import logging
from collections import defaultdict
from dataclasses import dataclass, field
from datetime import datetime, timedelta

from app.context.unified_context import ExecutionStatus, ExecutionType

logger = logging.getLogger(__name__)


@dataclass
class ExecutionMetrics:
    """å•ä¸ªæ‰§è¡Œè·¯å¾„çš„æ€§èƒ½æŒ‡æ ‡"""
    path: str
    total_count: int = 0
    success_count: int = 0
    failure_count: int = 0
    total_time_ms: float = 0.0  # æ€»æ‰§è¡Œæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    min_time_ms: float = float('inf')
    max_time_ms: float = 0.0
    error_types: dict[str, int] = field(default_factory=lambda: defaultdict(int))

    @property
    def success_rate(self) -> float:
        """æˆåŠŸçŽ‡ï¼ˆ0-1ï¼‰"""
        if self.total_count == 0:
            return 0.0
        return self.success_count / self.total_count

    @property
    def avg_time_ms(self) -> float:
        """å¹³å‡æ‰§è¡Œæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰"""
        if self.total_count == 0:
            return 0.0
        return self.total_time_ms / self.total_count

    def to_dict(self) -> dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "path": self.path,
            "total_count": self.total_count,
            "success_count": self.success_count,
            "failure_count": self.failure_count,
            "success_rate": f"{self.success_rate:.2%}",
            "avg_time_ms": f"{self.avg_time_ms:.2f}",
            "min_time_ms": f"{self.min_time_ms:.2f}" if self.min_time_ms != float('inf') else "N/A",
            "max_time_ms": f"{self.max_time_ms:.2f}",
            "error_types": dict(self.error_types),
        }


@dataclass
class ExecutionStats:
    """æ•´ä½“æ‰§è¡Œç»Ÿè®¡"""
    session_id: str
    start_time: datetime = field(default_factory=datetime.now)

    # æŒ‰æ‰§è¡Œè·¯å¾„çš„æŒ‡æ ‡
    skill_metrics: ExecutionMetrics = field(default_factory=lambda: ExecutionMetrics("skill"))
    mcp_metrics: ExecutionMetrics = field(default_factory=lambda: ExecutionMetrics("mcp"))
    llm_metrics: ExecutionMetrics = field(default_factory=lambda: ExecutionMetrics("llm"))

    # æ•´ä½“ç»Ÿè®¡
    total_executions: int = 0
    total_success: int = 0
    total_failure: int = 0

    def get_metric(self, execution_type: ExecutionType) -> ExecutionMetrics:
        """æ ¹æ®æ‰§è¡Œç±»åž‹èŽ·å–å¯¹åº”çš„æŒ‡æ ‡"""
        if execution_type == ExecutionType.SKILL:
            return self.skill_metrics
        elif execution_type == ExecutionType.MCP_CODE:
            return self.mcp_metrics
        elif execution_type == ExecutionType.LLM_REASONING:
            return self.llm_metrics
        else:
            raise ValueError(f"Unknown execution type: {execution_type}")

    @property
    def overall_success_rate(self) -> float:
        """æ•´ä½“æˆåŠŸçŽ‡"""
        if self.total_executions == 0:
            return 0.0
        return self.total_success / self.total_executions

    @property
    def total_time(self) -> timedelta:
        """æ€»æ‰§è¡Œæ—¶é—´"""
        return datetime.now() - self.start_time

    def to_dict(self) -> dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆä¾¿äºŽåºåˆ—åŒ–å’ŒæŠ¥å‘Šï¼‰"""
        return {
            "session_id": self.session_id,
            "start_time": self.start_time.isoformat(),
            "total_time": str(self.total_time),
            "total_executions": self.total_executions,
            "total_success": self.total_success,
            "total_failure": self.total_failure,
            "overall_success_rate": f"{self.overall_success_rate:.2%}",
            "skill_metrics": self.skill_metrics.to_dict(),
            "mcp_metrics": self.mcp_metrics.to_dict(),
            "llm_metrics": self.llm_metrics.to_dict(),
        }


class ExecutionMonitor:
    """
    Agent æ‰§è¡Œç›‘æŽ§å™¨

    ç”¨äºŽè®°å½•å’Œåˆ†æž Agent çš„æ‰§è¡Œæ€§èƒ½ï¼ŒåŒ…æ‹¬ï¼š
    - æ‰§è¡Œè·¯å¾„åˆ†å¸ƒ
    - æˆåŠŸçŽ‡ç»Ÿè®¡
    - å»¶è¿Ÿåˆ†æž
    - é”™è¯¯è¿½è¸ª
    """

    def __init__(self, session_id: str):
        """
        åˆå§‹åŒ–ç›‘æŽ§å™¨

        Args:
            session_id: Session ID
        """
        self.stats = ExecutionStats(session_id=session_id)
        self._execution_records: list[dict] = []

    def record_execution(
        self,
        execution_type: ExecutionType,
        status: ExecutionStatus,
        duration_ms: float,
        error_type: str | None = None,
        metadata: dict | None = None,
    ) -> None:
        """
        è®°å½•ä¸€æ¬¡æ‰§è¡Œ

        Args:
            execution_type: æ‰§è¡Œç±»åž‹ï¼ˆSKILL/MCP/LLMï¼‰
            status: æ‰§è¡ŒçŠ¶æ€ï¼ˆSUCCESS/FAILEDï¼‰
            duration_ms: æ‰§è¡Œè€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
            error_type: é”™è¯¯ç±»åž‹ï¼ˆå¦‚æžœå¤±è´¥ï¼‰
            metadata: é¢å¤–çš„å…ƒæ•°æ®
        """
        # æ›´æ–°æ€»ä½“ç»Ÿè®¡
        self.stats.total_executions += 1
        if status == ExecutionStatus.SUCCESS:
            self.stats.total_success += 1
        else:
            self.stats.total_failure += 1

        # èŽ·å–æ‰§è¡Œè·¯å¾„çš„æŒ‡æ ‡
        metrics = self.stats.get_metric(execution_type)
        metrics.total_count += 1

        if status == ExecutionStatus.SUCCESS:
            metrics.success_count += 1
        else:
            metrics.failure_count += 1
            if error_type:
                metrics.error_types[error_type] += 1

        # æ›´æ–°æ‰§è¡Œæ—¶é—´ç»Ÿè®¡
        metrics.total_time_ms += duration_ms
        metrics.min_time_ms = min(metrics.min_time_ms, duration_ms)
        metrics.max_time_ms = max(metrics.max_time_ms, duration_ms)

        # è®°å½•è¯¦ç»†ä¿¡æ¯
        record = {
            "timestamp": datetime.now().isoformat(),
            "execution_type": execution_type.value,
            "status": status.value,
            "duration_ms": duration_ms,
            "error_type": error_type,
            "metadata": metadata or {},
        }
        self._execution_records.append(record)

        logger.debug(f"Recorded execution: {execution_type.value} {status.value} {duration_ms:.2f}ms")

    def get_stats(self) -> ExecutionStats:
        """èŽ·å–å½“å‰ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats

    def get_path_distribution(self) -> dict[str, float]:
        """èŽ·å–æ‰§è¡Œè·¯å¾„åˆ†å¸ƒï¼ˆç™¾åˆ†æ¯”ï¼‰"""
        total = self.stats.total_executions
        if total == 0:
            return {"skill": 0.0, "mcp": 0.0, "llm": 0.0}

        return {
            "skill": self.stats.skill_metrics.total_count / total,
            "mcp": self.stats.mcp_metrics.total_count / total,
            "llm": self.stats.llm_metrics.total_count / total,
        }

    def get_success_rates(self) -> dict[str, float]:
        """æŒ‰æ‰§è¡Œè·¯å¾„èŽ·å–æˆåŠŸçŽ‡"""
        return {
            "skill": self.stats.skill_metrics.success_rate,
            "mcp": self.stats.mcp_metrics.success_rate,
            "llm": self.stats.llm_metrics.success_rate,
            "overall": self.stats.overall_success_rate,
        }

    def get_latency_stats(self) -> dict[str, dict[str, float]]:
        """èŽ·å–å»¶è¿Ÿç»Ÿè®¡ï¼ˆå•ä½ï¼šæ¯«ç§’ï¼‰"""
        def format_metrics(m: ExecutionMetrics) -> dict[str, float]:
            return {
                "avg_ms": round(m.avg_time_ms, 2),
                "min_ms": round(m.min_time_ms, 2) if m.min_time_ms != float('inf') else 0,
                "max_ms": round(m.max_time_ms, 2),
            }

        return {
            "skill": format_metrics(self.stats.skill_metrics),
            "mcp": format_metrics(self.stats.mcp_metrics),
            "llm": format_metrics(self.stats.llm_metrics),
        }

    def get_error_summary(self) -> dict[str, dict[str, int]]:
        """èŽ·å–é”™è¯¯æ‘˜è¦ï¼ˆæŒ‰æ‰§è¡Œè·¯å¾„åˆ†ç±»ï¼‰"""
        return {
            "skill": dict(self.stats.skill_metrics.error_types),
            "mcp": dict(self.stats.mcp_metrics.error_types),
            "llm": dict(self.stats.llm_metrics.error_types),
        }

    def generate_report(self) -> str:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        stats = self.stats
        distribution = self.get_path_distribution()
        success_rates = self.get_success_rates()
        latency = self.get_latency_stats()
        errors = self.get_error_summary()

        report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘            Agent Execution Performance Report                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ðŸ“Š OVERVIEW
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Session ID:           {stats.session_id}
Total Time:           {stats.total_time}
Total Executions:     {stats.total_executions}
Success:              {stats.total_success} ({stats.overall_success_rate:.1%})
Failure:              {stats.total_failure}

ðŸ“ˆ PATH DISTRIBUTION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Skill Path:           {distribution['skill']:>6.1%} ({stats.skill_metrics.total_count} executions)
MCP Path:             {distribution['mcp']:>6.1%} ({stats.mcp_metrics.total_count} executions)
LLM Path:             {distribution['llm']:>6.1%} ({stats.llm_metrics.total_count} executions)

âœ… SUCCESS RATES BY PATH
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Skill Path:           {success_rates['skill']:>6.1%} ({stats.skill_metrics.success_count}/{stats.skill_metrics.total_count})
MCP Path:             {success_rates['mcp']:>6.1%} ({stats.mcp_metrics.success_count}/{stats.mcp_metrics.total_count})
LLM Path:             {success_rates['llm']:>6.1%} ({stats.llm_metrics.success_count}/{stats.llm_metrics.total_count})
Overall:              {success_rates['overall']:>6.1%}

â±ï¸  LATENCY STATISTICS (milliseconds)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Skill Path:
  Average:            {latency['skill']['avg_ms']:>8.2f} ms
  Min:                {latency['skill']['min_ms']:>8.2f} ms
  Max:                {latency['skill']['max_ms']:>8.2f} ms

MCP Path:
  Average:            {latency['mcp']['avg_ms']:>8.2f} ms
  Min:                {latency['mcp']['min_ms']:>8.2f} ms
  Max:                {latency['mcp']['max_ms']:>8.2f} ms

LLM Path:
  Average:            {latency['llm']['avg_ms']:>8.2f} ms
  Min:                {latency['llm']['min_ms']:>8.2f} ms
  Max:                {latency['llm']['max_ms']:>8.2f} ms

âŒ ERROR ANALYSIS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Skill Errors:
{self._format_error_dict(errors['skill'])}
MCP Errors:
{self._format_error_dict(errors['mcp'])}
LLM Errors:
{self._format_error_dict(errors['llm'])}

ðŸŽ¯ KEY INSIGHTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Fastest Path:       Skill ({latency['skill']['avg_ms']:.1f}ms avg)
â€¢ Most Used Path:     {max(distribution, key=distribution.get).upper()}
â€¢ Most Reliable:      {max(success_rates, key=lambda x: success_rates[x] if x != 'overall' else 0).upper()} ({max([success_rates[k] for k in ['skill', 'mcp', 'llm']]):.1%})
"""

        return report

    @staticmethod
    def _format_error_dict(errors: dict[str, int]) -> str:
        """æ ¼å¼åŒ–é”™è¯¯å­—å…¸"""
        if not errors:
            return "  â€¢ No errors\n"

        lines = []
        for error_type, count in sorted(errors.items(), key=lambda x: -x[1]):
            lines.append(f"  â€¢ {error_type}: {count}")
        return "\n".join(lines) + "\n"

    def export_json(self, filepath: str) -> None:
        """å¯¼å‡ºç»Ÿè®¡æ•°æ®ä¸º JSON æ–‡ä»¶"""
        data = {
            "stats": self.stats.to_dict(),
            "path_distribution": self.get_path_distribution(),
            "success_rates": self.get_success_rates(),
            "latency_stats": self.get_latency_stats(),
            "error_summary": self.get_error_summary(),
            "execution_records": self._execution_records,
        }

        with open(filepath, 'w') as f:
            json.dump(data, f, indent=2, default=str)

        logger.info(f"Exported execution stats to {filepath}")

    def print_summary(self) -> None:
        """æ‰“å°æ‰§è¡Œæ‘˜è¦"""
        stats = self.stats
        print(f"\nâœ… Session: {stats.session_id}")
        print(f"   Total: {stats.total_executions} | Success: {stats.total_success} | Failed: {stats.total_failure}")
        print(f"   Success Rate: {stats.overall_success_rate:.1%}")

        # æŒ‰è·¯å¾„æ˜¾ç¤º
        for metrics in [stats.skill_metrics, stats.mcp_metrics, stats.llm_metrics]:
            if metrics.total_count > 0:
                print(f"   {metrics.path.upper()}: {metrics.total_count} executions, " +
                      f"{metrics.success_rate:.1%} success rate, {metrics.avg_time_ms:.2f}ms avg")


# å…¨å±€ç›‘æŽ§å™¨å®žä¾‹å­—å…¸ï¼ˆkey: session_idï¼‰
_monitors: dict[str, ExecutionMonitor] = {}


def get_execution_monitor(session_id: str) -> ExecutionMonitor:
    """èŽ·å–æˆ–åˆ›å»ºæ‰§è¡Œç›‘æŽ§å™¨ï¼ˆå•ä¾‹ï¼‰"""
    if session_id not in _monitors:
        _monitors[session_id] = ExecutionMonitor(session_id)
    return _monitors[session_id]


def clear_monitor(session_id: str) -> None:
    """æ¸…é™¤æŒ‡å®š session çš„ç›‘æŽ§å™¨"""
    if session_id in _monitors:
        del _monitors[session_id]


def clear_all_monitors() -> None:
    """æ¸…é™¤æ‰€æœ‰ç›‘æŽ§å™¨"""
    _monitors.clear()
